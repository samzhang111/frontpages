{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front pages of newspapers -- Initial discovery\n",
    "\n",
    "We have two datasets:\n",
    "* `frontpage_texts`, the text boxes extracted from pdfs of the front pages of newspapers, downloaded from the [Newseum](https://newseum.org/todaysfrontpages/)\n",
    "* `newspapers`, the metadata of the newspapers, also from the Newseum site.\n",
    "\n",
    "The text boxes contain interesting metadata for a given chunk of text, such as its bounding box, font, and size.\n",
    "\n",
    "This notebook will document some of the early exploratory attempts to understand the variety of the data, and to move toward performing an analysis of media coverage/bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql_table('frontpage_texts', 'postgres:///frontpages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fontface</th>\n",
       "      <th>fontsize</th>\n",
       "      <th>bbox_left</th>\n",
       "      <th>bbox_bottom</th>\n",
       "      <th>bbox_right</th>\n",
       "      <th>bbox_top</th>\n",
       "      <th>bbox_area</th>\n",
       "      <th>avg_character_area</th>\n",
       "      <th>percent_of_page</th>\n",
       "      <th>page</th>\n",
       "      <th>page_width</th>\n",
       "      <th>page_height</th>\n",
       "      <th>page_area</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>slug</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEYENNE – Education and \\nunderstanding are c...</td>\n",
       "      <td>UYJQTF+Dutch811BT-RomanD</td>\n",
       "      <td>11.894</td>\n",
       "      <td>355.199</td>\n",
       "      <td>565.665</td>\n",
       "      <td>495.693</td>\n",
       "      <td>814.413</td>\n",
       "      <td>34947.601512</td>\n",
       "      <td>53.988159</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>1</td>\n",
       "      <td>788.76</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>1135814.4</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>WY_WTE</td>\n",
       "      <td>146016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For more information\\n</td>\n",
       "      <td>IGKWHD+GriffithGothicCond-Ultra</td>\n",
       "      <td>18.224</td>\n",
       "      <td>508.720</td>\n",
       "      <td>892.167</td>\n",
       "      <td>635.040</td>\n",
       "      <td>910.391</td>\n",
       "      <td>2302.055680</td>\n",
       "      <td>120.001395</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>1</td>\n",
       "      <td>788.76</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>1135814.4</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>WY_WTE</td>\n",
       "      <td>146017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To learn more about the courthouse \\nexpansion...</td>\n",
       "      <td>HCVQTF+GriffithGothic-Bold</td>\n",
       "      <td>9.758</td>\n",
       "      <td>508.720</td>\n",
       "      <td>825.022</td>\n",
       "      <td>637.342</td>\n",
       "      <td>885.185</td>\n",
       "      <td>7738.285386</td>\n",
       "      <td>35.217527</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>1</td>\n",
       "      <td>788.76</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>1135814.4</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>WY_WTE</td>\n",
       "      <td>146018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There, you can read descriptions of all \\n44 p...</td>\n",
       "      <td>HCVQTF+GriffithGothic-Bold</td>\n",
       "      <td>9.758</td>\n",
       "      <td>508.720</td>\n",
       "      <td>798.859</td>\n",
       "      <td>637.929</td>\n",
       "      <td>818.698</td>\n",
       "      <td>2563.377351</td>\n",
       "      <td>33.936823</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>1</td>\n",
       "      <td>788.76</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>1135814.4</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>WY_WTE</td>\n",
       "      <td>146019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raising $18 million to provide ad-\\nditional s...</td>\n",
       "      <td>UYJQTF+Dutch811BT-RomanD</td>\n",
       "      <td>11.894</td>\n",
       "      <td>502.718</td>\n",
       "      <td>565.874</td>\n",
       "      <td>641.321</td>\n",
       "      <td>787.481</td>\n",
       "      <td>30715.395021</td>\n",
       "      <td>55.198782</td>\n",
       "      <td>0.027043</td>\n",
       "      <td>1</td>\n",
       "      <td>788.76</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>1135814.4</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>WY_WTE</td>\n",
       "      <td>146020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  CHEYENNE – Education and \\nunderstanding are c...   \n",
       "1                             For more information\\n   \n",
       "2  To learn more about the courthouse \\nexpansion...   \n",
       "3  There, you can read descriptions of all \\n44 p...   \n",
       "4  raising $18 million to provide ad-\\nditional s...   \n",
       "\n",
       "                          fontface  fontsize  bbox_left  bbox_bottom  \\\n",
       "0         UYJQTF+Dutch811BT-RomanD    11.894    355.199      565.665   \n",
       "1  IGKWHD+GriffithGothicCond-Ultra    18.224    508.720      892.167   \n",
       "2       HCVQTF+GriffithGothic-Bold     9.758    508.720      825.022   \n",
       "3       HCVQTF+GriffithGothic-Bold     9.758    508.720      798.859   \n",
       "4         UYJQTF+Dutch811BT-RomanD    11.894    502.718      565.874   \n",
       "\n",
       "   bbox_right  bbox_top     bbox_area  avg_character_area  percent_of_page  \\\n",
       "0     495.693   814.413  34947.601512           53.988159         0.030769   \n",
       "1     635.040   910.391   2302.055680          120.001395         0.002027   \n",
       "2     637.342   885.185   7738.285386           35.217527         0.006813   \n",
       "3     637.929   818.698   2563.377351           33.936823         0.002257   \n",
       "4     641.321   787.481  30715.395021           55.198782         0.027043   \n",
       "\n",
       "  page  page_width  page_height  page_area       date  day_of_week weekend  \\\n",
       "0    1      788.76       1440.0  1135814.4 2017-04-09            6    True   \n",
       "1    1      788.76       1440.0  1135814.4 2017-04-09            6    True   \n",
       "2    1      788.76       1440.0  1135814.4 2017-04-09            6    True   \n",
       "3    1      788.76       1440.0  1135814.4 2017-04-09            6    True   \n",
       "4    1      788.76       1440.0  1135814.4 2017-04-09            6    True   \n",
       "\n",
       "     slug      id  \n",
       "0  WY_WTE  146016  \n",
       "1  WY_WTE  146017  \n",
       "2  WY_WTE  146018  \n",
       "3  WY_WTE  146019  \n",
       "4  WY_WTE  146020  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some simple cleaning -- remove one character entries, and trailing newlines\n",
    "\n",
    "df['text'] = df['text'].str.strip()\n",
    "df = df[df['text'].str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_newspapers = pd.read_sql_table('newspapers', 'postgres:///frontpages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Centre</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.152336</td>\n",
       "      <td>-85.678963</td>\n",
       "      <td>AL_TP</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Post</td>\n",
       "      <td>http://www.epostpaper.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decatur</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.602890</td>\n",
       "      <td>-86.986511</td>\n",
       "      <td>AL_DD</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Decatur Daily</td>\n",
       "      <td>http://www.decaturdaily.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dothan</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.225517</td>\n",
       "      <td>-85.393631</td>\n",
       "      <td>AL_DE</td>\n",
       "      <td>AL</td>\n",
       "      <td>Dothan Eagle</td>\n",
       "      <td>http://www.dothaneagle.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Florence</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.799538</td>\n",
       "      <td>-87.677467</td>\n",
       "      <td>AL_TD</td>\n",
       "      <td>AL</td>\n",
       "      <td>TimesDaily</td>\n",
       "      <td>http://www.timesdaily.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Montgomery</td>\n",
       "      <td>USA</td>\n",
       "      <td>32.379051</td>\n",
       "      <td>-86.314224</td>\n",
       "      <td>AL_MA</td>\n",
       "      <td>AL</td>\n",
       "      <td>Montgomery Advertiser</td>\n",
       "      <td>http://www.montgomeryadvertiser.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city country   latitude  longitude   slug state  \\\n",
       "0      Centre     USA  34.152336 -85.678963  AL_TP    AL   \n",
       "1     Decatur     USA  34.602890 -86.986511  AL_DD    AL   \n",
       "2      Dothan     USA  31.225517 -85.393631  AL_DE    AL   \n",
       "3    Florence     USA  34.799538 -87.677467  AL_TD    AL   \n",
       "4  Montgomery     USA  32.379051 -86.314224  AL_MA    AL   \n",
       "\n",
       "                   title                              website  \n",
       "0               The Post            http://www.epostpaper.com  \n",
       "1      The Decatur Daily          http://www.decaturdaily.com  \n",
       "2           Dothan Eagle           http://www.dothaneagle.com  \n",
       "3             TimesDaily            http://www.timesdaily.com  \n",
       "4  Montgomery Advertiser  http://www.montgomeryadvertiser.com  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newspapers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have metadata for 1031 newspapers.\n",
      "\n",
      "There are 80 total countries represented. The top 5 are:\n",
      "USA       685\n",
      "Brazil     60\n",
      "Canada     38\n",
      "India      21\n",
      "Mexico     19\n",
      "Name: country, dtype: int64.\n",
      "\n",
      "Within the US, there is representation from 51 states. The states with the most newspapers are:\n",
      "NY    55\n",
      "CA    52\n",
      "PA    44\n",
      "FL    33\n",
      "TX    28\n",
      "Name: state, dtype: int64\n",
      "\n",
      "And the least:\n",
      "AK    3\n",
      "ND    3\n",
      "NV    3\n",
      "DE    2\n",
      "HI    1\n",
      "Name: state, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_newspapers_df = df_newspapers[df_newspapers.country == 'USA']\n",
    "print('''We have metadata for {} newspapers.\n",
    "\n",
    "There are {} total countries represented. The top 5 are:\n",
    "{}.\n",
    "\n",
    "Within the US, there is representation from {} states. The states with the most newspapers are:\n",
    "{}\n",
    "\n",
    "And the least:\n",
    "{}\n",
    "\n",
    "'''.format(\n",
    "    df_newspapers.shape[0],\n",
    "    df_newspapers.country.nunique(),\n",
    "    df_newspapers.country.value_counts()[:5],\n",
    "    us_newspapers_df.state.nunique(),\n",
    "    us_newspapers_df.state.value_counts()[:5],\n",
    "    us_newspapers_df.state.value_counts()[-5:],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, there are:\n",
      "\n",
      "156335 rows of text\n",
      "8 days of scrapes\n",
      "  (earliest: 2017-04-01 00:00:00 \n",
      "   latest  : 2017-04-10 00:00:00)\n",
      "577 total newspapers (not all the pdfs were extractable).\n",
      "\n",
      "Filtering down to the US, there are now:\n",
      "386 newspapers\n",
      "109669 rows of text\n",
      "\n",
      "For those newspapers that are available in the US, there are:\n",
      "49 states\n",
      "states with most newspapers:\n",
      "CA    23\n",
      "PA    22\n",
      "TX    19\n",
      "IN    19\n",
      "FL    16\n",
      "Name: state, dtype: int64\n",
      "\n",
      "with least:\n",
      "RI    2\n",
      "NE    2\n",
      "DE    2\n",
      "AK    1\n",
      "NV    1\n",
      "Name: state, dtype: int64\n",
      "\n",
      "with none:\n",
      "{'', 'NH', 'HI'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us = df[df.slug.isin(set(us_newspapers_df.slug))]\n",
    "newspapers_in_df = df_newspapers[df_newspapers.slug.isin(set(df_us.slug))]\n",
    "\n",
    "print('''Currently, there are:\n",
    "\n",
    "{} rows of text\n",
    "{} days of scrapes\n",
    "  (earliest: {} \n",
    "   latest  : {})\n",
    "{} total newspapers (not all the pdfs were extractable).\n",
    "\n",
    "Filtering down to the US, there are now:\n",
    "{} newspapers\n",
    "{} rows of text\n",
    "\n",
    "For those newspapers that are available in the US, there are:\n",
    "{} states\n",
    "states with most newspapers:\n",
    "{}\n",
    "\n",
    "with least:\n",
    "{}\n",
    "\n",
    "with none:\n",
    "{}\n",
    "'''.format(\n",
    "    df.shape[0],\n",
    "    df.date.nunique(),\n",
    "    df.date.min(),\n",
    "    df.date.max(),\n",
    "    df.slug.nunique(),\n",
    "\n",
    "    df_us.slug.nunique(),\n",
    "    df_us.shape[0],\n",
    "    \n",
    "    newspapers_in_df.state.nunique(),\n",
    "    newspapers_in_df.state.value_counts()[:5],\n",
    "    newspapers_in_df.state.value_counts()[-5:],\n",
    "    set(df_newspapers.state) - set(newspapers_in_df.state)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonts are often written in a format like this: UYJQTF+Dutch811BT-RomanD.\n",
      "\n",
      "Out of 156335 rows...\n",
      "156335 of the fonts have non-empty text\n",
      "109107 of the fonts have a '+'\n",
      "135940 of the fonts have a '-'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''Fonts are often written in a format like this: {}.\n",
    "\n",
    "Out of {} rows...\n",
    "{} of the fonts have non-empty text\n",
    "{} of the fonts have a '+'\n",
    "{} of the fonts have a '-'\n",
    "'''.format(\n",
    "    df.fontface.iloc[0],\n",
    "    df.shape[0],\n",
    "    (df.fontface.str.len() > 0).sum(),\n",
    "    df.fontface.str.contains('\\+').sum(),\n",
    "    df.fontface.str.contains('-').sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This seems to mean that we can break apart the font into:\n",
      "[optional-leading-thing]+[font-family]-[font-weight]\n",
      "\n",
      "After doing that,\n",
      "There are...\n",
      "1694 unique font families\n",
      "1278 unique font weights\n",
      "19359 unique optional-leading-things\n"
     ]
    }
   ],
   "source": [
    "print('''This seems to mean that we can break apart the font into:\n",
    "[optional-leading-thing]+[font-family]-[font-weight]\n",
    "''')\n",
    "\n",
    "font_partition = df.fontface.str.rpartition('+')\n",
    "df['font_family_weight'] = font_partition[2]\n",
    "\n",
    "font_family_partition = df['font_family_weight'].str.partition('-')\n",
    "\n",
    "df['font_leading_thing'] = font_partition[0]\n",
    "df['font_family'] = font_family_partition[0]\n",
    "df['font_weight'] = font_family_partition[2]\n",
    "\n",
    "print('''After doing that,\n",
    "There are...\n",
    "{} unique font families\n",
    "{} unique font weights\n",
    "{} unique optional-leading-things'''.format(\n",
    "    df.font_family.nunique(),\n",
    "    df.font_weight.nunique(),\n",
    "    df.font_leading_thing.nunique()\n",
    "))\n",
    "\n",
    "df_us = df[df.slug.isin(set(us_newspapers_df.slug))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denver Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Denver</td>\n",
       "      <td>USA</td>\n",
       "      <td>39.741684</td>\n",
       "      <td>-104.987366</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>CO</td>\n",
       "      <td>The Denver Post</td>\n",
       "      <td>http://www.denverpost.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city country   latitude   longitude   slug state            title  \\\n",
       "54  Denver     USA  39.741684 -104.987366  CO_DP    CO  The Denver Post   \n",
       "\n",
       "                      website  \n",
       "54  http://www.denverpost.com  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do something with a Denver paper\n",
    "\n",
    "df_newspapers[df_newspapers.city == 'Denver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 days of scraped Denver Post front pages.\n",
      "\n",
      "We have 47 unique font-weight combos. Here is a mapping of each font family to their min, average, and max font size.\n",
      "\n",
      "                                count     min      max        avg\n",
      "font_family_weight                                               \n",
      "Agenda-Bold                       1.0  31.870   31.870  31.870000\n",
      "AkzidenzGroteskBE-LightEx         1.0   4.354    4.354   4.354000\n",
      "AkzidenzGroteskBE-MdEx            2.0   6.244   13.537   9.890500\n",
      "Arial-Black                       2.0  26.410   26.410  26.410000\n",
      "Arial-BoldItalicMT                2.0   9.469    9.469   9.469000\n",
      "Avenir-Heavy                      3.0   4.853    4.853   4.853000\n",
      "AzoSansUber-Regular               2.0  19.680   24.928  22.304000\n",
      "CastleT-Ultr                      2.0  39.039   39.039  39.039000\n",
      "DPPiFont                          4.0   8.504    8.504   8.504000\n",
      "Gotham-Thin                       1.0   8.839    8.839   8.839000\n",
      "MyriadPro-Bold                    2.0  27.258   39.648  33.453000\n",
      "MyriadPro-BoldIt                  4.0  17.318   27.845  22.674250\n",
      "MyriadPro-It                      2.0  13.233   13.534  13.383500\n",
      "MyriadPro-Regular                22.0   9.616   10.097  10.053273\n",
      "MyriadPro-SemiboldIt              2.0  13.442   18.330  15.886000\n",
      "NewBaskervilleStd-Bold            1.0  15.314   15.314  15.314000\n",
      "NewBaskervilleStd-Italic          1.0  15.340   15.340  15.340000\n",
      "NewBaskervilleStd-Roman           1.0  12.958   12.958  12.958000\n",
      "PoynterOSDisplay-Bold             7.0  12.551   22.820  16.446714\n",
      "PoynterOSDisplay-Italic           8.0  16.605   16.605  16.605000\n",
      "PoynterOSDisplay-Roman           10.0  16.740   73.656  40.700500\n",
      "PoynterOSDisplay-Semibold         4.0  15.778   40.572  29.865500\n",
      "PoynterOSDisplay-SemiboldItal     1.0  19.176   19.176  19.176000\n",
      "PoynterOSDisplayNarrow-Bold      10.0  24.904  104.144  66.278600\n",
      "PoynterOSDisplayNarrow-Semibld    9.0  23.366   41.366  31.167333\n",
      "PoynterOSTextThree-Bold          13.0  10.214   10.214  10.214000\n",
      "PoynterOSTextThree-Italic        13.0   8.344   10.013   9.242692\n",
      "PoynterOSTextThree-Roman         54.0  10.609   13.112  10.748056\n",
      "PoynterOSTextThree-SemiBld        9.0  10.671   10.671  10.671000\n",
      "PoynterOSTextTwoL-Italic          2.0   8.302    8.302   8.302000\n",
      "Sun-Bold                         23.0  10.860   11.946  11.473826\n",
      "Sun-ExtraBold                     2.0  13.188   13.668  13.428000\n",
      "Sun-Regular                       1.0   9.496    9.496   9.496000\n",
      "Sun-SemiBold                     32.0  10.114   36.584  19.078688\n",
      "SunSC-Bold                        7.0   9.514    9.514   9.514000\n",
      "SunSC-ExtraBold                  23.0   8.680    8.680   8.680000\n",
      "SunSC-Light                       1.0  11.385   11.385  11.385000\n",
      "SunSC-Regular                     8.0   7.322   10.460   7.714250\n",
      "SunSC-SemiBold                    2.0   9.936   16.912  13.424000\n",
      "TheSerifBold-Caps                 2.0  14.454   14.454  14.454000\n",
      "TheSerifExtraBold-Plain           4.0  18.007   29.466  25.373500\n",
      "TheSerifLight-Plain              17.0  22.232   22.232  22.232000\n",
      "TradeGothicLTStd-Cn18             2.0   2.664    8.162   5.413000\n",
      "Univers-Bold                      2.0  11.880   38.265  25.072500\n",
      "UniversLTStd                      1.0  10.773   10.773  10.773000\n",
      "UniversLTStd-Bold                 1.0  22.572   22.572  22.572000\n",
      "Veneer                            3.0  43.472   71.978  52.974000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_denver_post = df_us[df_us.slug == 'CO_DP']\n",
    "\n",
    "font_stats = df_denver_post.groupby(['font_family_weight']).fontsize.agg({'count': len, 'min': np.min, 'max': np.max, 'avg': np.mean})\n",
    "\n",
    "print('''We have {} days of scraped Denver Post front pages.\n",
    "\n",
    "We have {} unique font-weight combos. Here is a mapping of each font family to their min, average, and max font size.\n",
    "\n",
    "{}\n",
    "'''.format(\n",
    "    df_denver_post.date.nunique(),\n",
    "    df_denver_post.groupby(['font_family_weight']).first().shape[0],\n",
    "    font_stats\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonts by number of days on which they appear\n",
      "\n",
      "font_family_weight\n",
      "PoynterOSDisplayNarrow-Bold       8\n",
      "SunSC-Regular                     8\n",
      "PoynterOSTextThree-Roman          8\n",
      "Sun-SemiBold                      8\n",
      "SunSC-ExtraBold                   8\n",
      "PoynterOSDisplayNarrow-Semibld    8\n",
      "PoynterOSDisplay-Italic           8\n",
      "Sun-Bold                          8\n",
      "TheSerifLight-Plain               7\n",
      "PoynterOSTextThree-Bold           7\n",
      "PoynterOSTextThree-Italic         7\n",
      "PoynterOSTextThree-SemiBld        6\n",
      "PoynterOSDisplay-Roman            6\n",
      "PoynterOSDisplay-Bold             6\n",
      "PoynterOSDisplay-Semibold         4\n",
      "DPPiFont                          4\n",
      "SunSC-Bold                        4\n",
      "MyriadPro-Regular                 3\n",
      "TheSerifExtraBold-Plain           3\n",
      "MyriadPro-It                      2\n",
      "Sun-ExtraBold                     2\n",
      "Arial-BoldItalicMT                2\n",
      "SunSC-SemiBold                    2\n",
      "CastleT-Ultr                      2\n",
      "TheSerifBold-Caps                 2\n",
      "Arial-Black                       2\n",
      "MyriadPro-Bold                    2\n",
      "MyriadPro-BoldIt                  2\n",
      "MyriadPro-SemiboldIt              2\n",
      "AzoSansUber-Regular               2\n",
      "Avenir-Heavy                      1\n",
      "AkzidenzGroteskBE-MdEx            1\n",
      "AkzidenzGroteskBE-LightEx         1\n",
      "Gotham-Thin                       1\n",
      "Veneer                            1\n",
      "NewBaskervilleStd-Bold            1\n",
      "NewBaskervilleStd-Italic          1\n",
      "NewBaskervilleStd-Roman           1\n",
      "PoynterOSDisplay-SemiboldItal     1\n",
      "UniversLTStd-Bold                 1\n",
      "PoynterOSTextTwoL-Italic          1\n",
      "Sun-Regular                       1\n",
      "SunSC-Light                       1\n",
      "TradeGothicLTStd-Cn18             1\n",
      "Univers-Bold                      1\n",
      "UniversLTStd                      1\n",
      "Agenda-Bold                       1\n",
      "Name: date, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "font_days = df_denver_post.groupby(['font_family_weight']).date.nunique().sort_values(ascending=False)\n",
    "\n",
    "print('''Fonts by number of days on which they appear\n",
    "\n",
    "{}\n",
    "'''.format(\n",
    "    font_days\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x113c202e8>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEVCAYAAAAfCXWSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncaibmVWIQ93MrEIc6mZmFeJQNzOrkP8PGyiHV8Sq2SYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146504d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font_stats['days_present'] = font_days\n",
    "\n",
    "plt.suptitle('Number of days a font appears, vs. total font appearances')\n",
    "plt.scatter(font_stats.days_present, font_stats['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fontface</th>\n",
       "      <th>fontsize</th>\n",
       "      <th>bbox_left</th>\n",
       "      <th>bbox_bottom</th>\n",
       "      <th>bbox_right</th>\n",
       "      <th>bbox_top</th>\n",
       "      <th>bbox_area</th>\n",
       "      <th>avg_character_area</th>\n",
       "      <th>percent_of_page</th>\n",
       "      <th>...</th>\n",
       "      <th>page_area</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>slug</th>\n",
       "      <th>id</th>\n",
       "      <th>font_family_weight</th>\n",
       "      <th>font_leading_thing</th>\n",
       "      <th>font_family</th>\n",
       "      <th>font_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>Slope\\nfaces\\nthreat</td>\n",
       "      <td>OLOAGM+PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>61.128</td>\n",
       "      <td>573.00</td>\n",
       "      <td>1032.420</td>\n",
       "      <td>703.680</td>\n",
       "      <td>1201.548</td>\n",
       "      <td>22101.647040</td>\n",
       "      <td>1401.443451</td>\n",
       "      <td>0.020073</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>149534</td>\n",
       "      <td>PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>OLOAGM</td>\n",
       "      <td>PoynterOSDisplayNarrow</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>WHO’S GOT THE BUTTON?</td>\n",
       "      <td>OLODGD+PoynterOSDisplay-Semibold</td>\n",
       "      <td>40.572</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1227.980</td>\n",
       "      <td>479.055</td>\n",
       "      <td>1268.552</td>\n",
       "      <td>18705.923460</td>\n",
       "      <td>997.343158</td>\n",
       "      <td>0.016989</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>149518</td>\n",
       "      <td>PoynterOSDisplay-Semibold</td>\n",
       "      <td>OLODGD</td>\n",
       "      <td>PoynterOSDisplay</td>\n",
       "      <td>Semibold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>303-328-3</td>\n",
       "      <td>OLOFEB+CastleT-Ultr</td>\n",
       "      <td>39.039</td>\n",
       "      <td>29.28</td>\n",
       "      <td>57.350</td>\n",
       "      <td>188.483</td>\n",
       "      <td>96.389</td>\n",
       "      <td>6215.125917</td>\n",
       "      <td>667.861861</td>\n",
       "      <td>0.005645</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>149546</td>\n",
       "      <td>CastleT-Ultr</td>\n",
       "      <td>OLOFEB</td>\n",
       "      <td>CastleT</td>\n",
       "      <td>Ultr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>H E A L T H C A R EWest</td>\n",
       "      <td>OLOAEE+Sun-Bold</td>\n",
       "      <td>11.946</td>\n",
       "      <td>573.00</td>\n",
       "      <td>1194.420</td>\n",
       "      <td>678.527</td>\n",
       "      <td>1258.446</td>\n",
       "      <td>6756.471702</td>\n",
       "      <td>545.748008</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>149533</td>\n",
       "      <td>Sun-Bold</td>\n",
       "      <td>OLOAEE</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>NUGGETS ELIMINATED BY LOSS TO THUNDER » 1B</td>\n",
       "      <td>OLOADA+Sun-SemiBold</td>\n",
       "      <td>32.280</td>\n",
       "      <td>97.40</td>\n",
       "      <td>1432.230</td>\n",
       "      <td>659.249</td>\n",
       "      <td>1464.510</td>\n",
       "      <td>18136.485720</td>\n",
       "      <td>477.634245</td>\n",
       "      <td>0.016472</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>149513</td>\n",
       "      <td>Sun-SemiBold</td>\n",
       "      <td>OLOADA</td>\n",
       "      <td>Sun</td>\n",
       "      <td>SemiBold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150814</th>\n",
       "      <td>Train sidetracked\\nby costly problems</td>\n",
       "      <td>LJJAKB+PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>76.976</td>\n",
       "      <td>192.46</td>\n",
       "      <td>1039.602</td>\n",
       "      <td>699.747</td>\n",
       "      <td>1180.566</td>\n",
       "      <td>71509.204668</td>\n",
       "      <td>2357.592062</td>\n",
       "      <td>0.064946</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>132204</td>\n",
       "      <td>PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>LJJAKB</td>\n",
       "      <td>PoynterOSDisplayNarrow</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150795</th>\n",
       "      <td>PIONEERS WIN 8TH NCAA\\nHOCKEY CHAMPIONSHIP» 1C</td>\n",
       "      <td>LJJAJA+Sun-SemiBold</td>\n",
       "      <td>34.432</td>\n",
       "      <td>212.25</td>\n",
       "      <td>1374.810</td>\n",
       "      <td>531.637</td>\n",
       "      <td>1439.242</td>\n",
       "      <td>20578.743184</td>\n",
       "      <td>559.303913</td>\n",
       "      <td>0.018690</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>132185</td>\n",
       "      <td>Sun-SemiBold</td>\n",
       "      <td>LJJAJA</td>\n",
       "      <td>Sun</td>\n",
       "      <td>SemiBold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150803</th>\n",
       "      <td>Fresh strike\\non town hit\\nby deadly\\nchemicals</td>\n",
       "      <td>LJJCDM+PoynterOSDisplayNarrow-Semibld</td>\n",
       "      <td>33.540</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1053.800</td>\n",
       "      <td>151.800</td>\n",
       "      <td>1177.340</td>\n",
       "      <td>16529.652000</td>\n",
       "      <td>439.926957</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>132193</td>\n",
       "      <td>PoynterOSDisplayNarrow-Semibld</td>\n",
       "      <td>LJJCDM</td>\n",
       "      <td>PoynterOSDisplayNarrow</td>\n",
       "      <td>Semibld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150813</th>\n",
       "      <td>A-Line Anniversary</td>\n",
       "      <td>LJJAJO+TheSerifExtraBold-Plain</td>\n",
       "      <td>29.466</td>\n",
       "      <td>192.46</td>\n",
       "      <td>1169.370</td>\n",
       "      <td>356.042</td>\n",
       "      <td>1198.836</td>\n",
       "      <td>4820.107212</td>\n",
       "      <td>279.140084</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>132203</td>\n",
       "      <td>TheSerifExtraBold-Plain</td>\n",
       "      <td>LJJAJO</td>\n",
       "      <td>TheSerifExtraBold</td>\n",
       "      <td>Plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150791</th>\n",
       "      <td>M EM B E R S</td>\n",
       "      <td>LJJAJF+TheSerifLight-Plain</td>\n",
       "      <td>22.232</td>\n",
       "      <td>35.40</td>\n",
       "      <td>1440.360</td>\n",
       "      <td>104.252</td>\n",
       "      <td>1462.592</td>\n",
       "      <td>1530.717664</td>\n",
       "      <td>202.666912</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>132181</td>\n",
       "      <td>TheSerifLight-Plain</td>\n",
       "      <td>LJJAJF</td>\n",
       "      <td>TheSerifLight</td>\n",
       "      <td>Plain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "3520                               Slope\\nfaces\\nthreat   \n",
       "3501                              WHO’S GOT THE BUTTON?   \n",
       "3508                                          303-328-3   \n",
       "3519                            H E A L T H C A R EWest   \n",
       "3496         NUGGETS ELIMINATED BY LOSS TO THUNDER » 1B   \n",
       "150814            Train sidetracked\\nby costly problems   \n",
       "150795   PIONEERS WIN 8TH NCAA\\nHOCKEY CHAMPIONSHIP» 1C   \n",
       "150803  Fresh strike\\non town hit\\nby deadly\\nchemicals   \n",
       "150813                               A-Line Anniversary   \n",
       "150791                                     M EM B E R S   \n",
       "\n",
       "                                     fontface  fontsize  bbox_left  \\\n",
       "3520       OLOAGM+PoynterOSDisplayNarrow-Bold    61.128     573.00   \n",
       "3501         OLODGD+PoynterOSDisplay-Semibold    40.572      18.00   \n",
       "3508                      OLOFEB+CastleT-Ultr    39.039      29.28   \n",
       "3519                          OLOAEE+Sun-Bold    11.946     573.00   \n",
       "3496                      OLOADA+Sun-SemiBold    32.280      97.40   \n",
       "150814     LJJAKB+PoynterOSDisplayNarrow-Bold    76.976     192.46   \n",
       "150795                    LJJAJA+Sun-SemiBold    34.432     212.25   \n",
       "150803  LJJCDM+PoynterOSDisplayNarrow-Semibld    33.540      18.00   \n",
       "150813         LJJAJO+TheSerifExtraBold-Plain    29.466     192.46   \n",
       "150791             LJJAJF+TheSerifLight-Plain    22.232      35.40   \n",
       "\n",
       "        bbox_bottom  bbox_right  bbox_top     bbox_area  avg_character_area  \\\n",
       "3520       1032.420     703.680  1201.548  22101.647040         1401.443451   \n",
       "3501       1227.980     479.055  1268.552  18705.923460          997.343158   \n",
       "3508         57.350     188.483    96.389   6215.125917          667.861861   \n",
       "3519       1194.420     678.527  1258.446   6756.471702          545.748008   \n",
       "3496       1432.230     659.249  1464.510  18136.485720          477.634245   \n",
       "150814     1039.602     699.747  1180.566  71509.204668         2357.592062   \n",
       "150795     1374.810     531.637  1439.242  20578.743184          559.303913   \n",
       "150803     1053.800     151.800  1177.340  16529.652000          439.926957   \n",
       "150813     1169.370     356.042  1198.836   4820.107212          279.140084   \n",
       "150791     1440.360     104.252  1462.592   1530.717664          202.666912   \n",
       "\n",
       "        percent_of_page     ...      page_area       date  day_of_week  \\\n",
       "3520           0.020073     ...      1101056.0 2017-04-10            0   \n",
       "3501           0.016989     ...      1101056.0 2017-04-10            0   \n",
       "3508           0.005645     ...      1101056.0 2017-04-10            0   \n",
       "3519           0.006136     ...      1101056.0 2017-04-10            0   \n",
       "3496           0.016472     ...      1101056.0 2017-04-10            0   \n",
       "150814         0.064946     ...      1101056.0 2017-04-09            6   \n",
       "150795         0.018690     ...      1101056.0 2017-04-09            6   \n",
       "150803         0.015013     ...      1101056.0 2017-04-09            6   \n",
       "150813         0.004378     ...      1101056.0 2017-04-09            6   \n",
       "150791         0.001390     ...      1101056.0 2017-04-09            6   \n",
       "\n",
       "        weekend   slug      id              font_family_weight  \\\n",
       "3520      False  CO_DP  149534     PoynterOSDisplayNarrow-Bold   \n",
       "3501      False  CO_DP  149518       PoynterOSDisplay-Semibold   \n",
       "3508      False  CO_DP  149546                    CastleT-Ultr   \n",
       "3519      False  CO_DP  149533                        Sun-Bold   \n",
       "3496      False  CO_DP  149513                    Sun-SemiBold   \n",
       "150814     True  CO_DP  132204     PoynterOSDisplayNarrow-Bold   \n",
       "150795     True  CO_DP  132185                    Sun-SemiBold   \n",
       "150803     True  CO_DP  132193  PoynterOSDisplayNarrow-Semibld   \n",
       "150813     True  CO_DP  132203         TheSerifExtraBold-Plain   \n",
       "150791     True  CO_DP  132181             TheSerifLight-Plain   \n",
       "\n",
       "       font_leading_thing             font_family font_weight  \n",
       "3520               OLOAGM  PoynterOSDisplayNarrow        Bold  \n",
       "3501               OLODGD        PoynterOSDisplay    Semibold  \n",
       "3508               OLOFEB                 CastleT        Ultr  \n",
       "3519               OLOAEE                     Sun        Bold  \n",
       "3496               OLOADA                     Sun    SemiBold  \n",
       "150814             LJJAKB  PoynterOSDisplayNarrow        Bold  \n",
       "150795             LJJAJA                     Sun    SemiBold  \n",
       "150803             LJJCDM  PoynterOSDisplayNarrow     Semibld  \n",
       "150813             LJJAJO       TheSerifExtraBold       Plain  \n",
       "150791             LJJAJF           TheSerifLight       Plain  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_denver_post.sort_values(['date', 'avg_character_area'], ascending=False).groupby('date').head(5).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram \"percent of page\" analysis\n",
    "\n",
    "Given an unigram like \"Syria\", how much of a given front page does it occupy? \n",
    "\n",
    "### Notes\n",
    "\n",
    "We will consider the entire text block that contains the unigram to be related to that unigram. For example, the entire headline of \"US BOMBS SYRIA\" will be counted as space devoted toward \"Syria\". Likewise, a lengthy front-page article that mentions \"Syria\" in it will (naively, perhaps) be considered 100% about Syria.\n",
    "\n",
    "We're assuming that search queries will be proper nouns, so we're not going to perform any stemming or lemmatizing.\n",
    "\n",
    "### Followup approaches\n",
    "\n",
    "Some newspapers contain more and smaller text, like the NYT, compared to tabloids where words are written extremely largely across the surface. This may still be of interest -- we do want to acknowledge the space devoted to \"Syria\" if it is splashed across the front of the tabloid -- but we may also want to develop a measure of relative importance so that a top-of-banner headline is weighted equally across all newspapers.\n",
    "\n",
    "This approach does not touch on probabilistic topic modeling yet -- these are only direct matches.\n",
    "\n",
    "We will also want to develop a method to link a headline with an article, so that a headline like \"BOOTS ON THE GROUND\" could possibly be linked to the followup article on Syria. This would also allow us to do some tangential but interesting accounts of which Associated Press articles get republished the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text preprocessing, we consider a few cases:\n",
      "\n",
      "* Newlines should be stripped\n",
      "* Everything should be lower-cased\n",
      "* We should return a tokenized list\n",
      "* Tokens without a certain number of ascii characters (US-English analysis for now) will be rejected\n",
      "\n",
      "The extraction from PDFs still contains word-continuations across line breaks.\n",
      "For now, we'll consider all lines that end with \"-\" as continuations, and\n",
      "link the text from before and after.\n",
      "\n",
      "Newlines without continuations will be replaced with spaces.\n",
      "\n",
      "Examples:\n",
      "[('Hel-\\nlo, bye\\nnow\\n', ['hello', 'bye', 'now']),\n",
      " ('Salvador to the United States in 2003. Daniel Brenner, Special to The '\n",
      "  'Denver Post',\n",
      "  ['salvador',\n",
      "   'the',\n",
      "   'united',\n",
      "   'states',\n",
      "   'daniel',\n",
      "   'brenner',\n",
      "   'special',\n",
      "   'the',\n",
      "   'denver',\n",
      "   'post']),\n",
      " ('By Mary Clare Jalonick\\nand Erica Werner\\nThe Associated Press',\n",
      "  ['mary',\n",
      "   'clare',\n",
      "   'jalonick',\n",
      "   'and',\n",
      "   'erica',\n",
      "   'werner',\n",
      "   'the',\n",
      "   'associated',\n",
      "   'press']),\n",
      " ('By Hope Yen The Associated Press',\n",
      "  ['hope', 'yen', 'the', 'associated', 'press'])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import string\n",
    "\n",
    "chars = set(string.ascii_letters)\n",
    "\n",
    "def include_word(word):\n",
    "    return sum([c in chars for c in word]) >= 3\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lowered = text.strip().lower()\n",
    "    lowered = ''.join(lowered.split('-\\n'))\n",
    "    lowered = lowered.replace('\\n', ' ')\n",
    "    words = word_tokenize(lowered)\n",
    "    filtered_words = [word for word in words if include_word(word)]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "def bag_of_words(text):\n",
    "    '''Literally, this returns a set of the bag of words for fast single-token searches'''\n",
    "    return set(preprocess_text(text))\n",
    "\n",
    "def preprocess_all(texts):\n",
    "    for text in texts:\n",
    "        yield text, preprocess_text(text)\n",
    "\n",
    "print('''For text preprocessing, we consider a few cases:\n",
    "\n",
    "* Newlines should be stripped\n",
    "* Everything should be lower-cased\n",
    "* We should return a tokenized list\n",
    "* Tokens without a certain number of ascii characters (US-English analysis for now) will be rejected\n",
    "\n",
    "The extraction from PDFs still contains word-continuations across line breaks.\n",
    "For now, we'll consider all lines that end with \"-\" as continuations, and\n",
    "link the text from before and after.\n",
    "\n",
    "Newlines without continuations will be replaced with spaces.\n",
    "\n",
    "Examples:\n",
    "{}\n",
    "'''.format(\n",
    "    pprint.pformat(list(preprocess_all([\n",
    "        'Hel-\\nlo, bye\\nnow\\n',\n",
    "         *df_denver_post.text.sample(3)\n",
    "    ])))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_us['bow'] = df_us.text.apply(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we write a method to get the percent of page that a unigram occupies, for a particular front page.\n",
      "\n",
      "Syria, Denver Post, latest day: 0.04013226803904611\n",
      "garbage input, should be 0: 0\n"
     ]
    }
   ],
   "source": [
    "df_denver_post_latest = df_us[(df_us.slug == 'CO_DP') & (df_us.date == df_us.date.max())]\n",
    "\n",
    "def percent_of_page(unigram, one_paper_df):\n",
    "    unigram = unigram.lower().strip()\n",
    "    lines_with_unigram = one_paper_df[one_paper_df.bow.apply(lambda bag: unigram in bag)]\n",
    "    \n",
    "    return lines_with_unigram.percent_of_page.sum()\n",
    "\n",
    "print('''Now we write a method to get the percent of page that a unigram occupies, for a particular front page.\n",
    "\n",
    "Syria, Denver Post, latest day: {}\n",
    "garbage input, should be 0: {}'''.format(\n",
    "    percent_of_page('Syria', df_denver_post_latest),\n",
    "    percent_of_page('asdflkjasdflasdfkjasdf', df_denver_post_latest)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run this method across all the newspapers, across all days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newspapers with >3 days: 291\n",
      "\n",
      "(Number of total newspapers: 386)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter down to newspapers with entries with more than 3 days\n",
    "days_of_newspapers = df_us.groupby('slug').date.nunique()\n",
    "\n",
    "df_us_3plus = df_us[df_us.slug.isin(set(days_of_newspapers[days_of_newspapers > 3].index))]\n",
    "\n",
    "print('''Number of newspapers with >3 days: {}\n",
    "\n",
    "(Number of total newspapers: {})\n",
    "'''.format(\n",
    "    df_us_3plus.slug.nunique(),\n",
    "    df_us.slug.nunique()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unigram_percent_of_page(query, dataframe):\n",
    "    return dataframe.groupby(['slug', 'date']).apply(partial(percent_of_page, query))\n",
    "\n",
    "def _reshape_percent_of_day_series(percent_of_page):\n",
    "    return percent_of_page.reset_index().rename(columns={0: 'percent_of_page'})\n",
    "\n",
    "def percent_of_page_by_day(percent_of_page_df):\n",
    "    return _reshape_percent_of_day_series(percent_of_page_df).groupby('date').percent_of_page.mean()\n",
    "\n",
    "def percent_of_papers_with_mention(percent_of_page_df, threshold=0):\n",
    "    percents_by_paper_date = _reshape_percent_of_day_series(percent_of_page_df)\n",
    "    greater_than_thresh = (percents_by_paper_date.groupby(['slug', 'date']).percent_of_page.max() > threshold).reset_index()\n",
    "    \n",
    "    return greater_than_thresh.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average mentions per day\n",
    "syria_results = unigram_percent_of_page('Syria', df_us_3plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of papers that mentioned Syria by day:\n",
      "            percent_of_page\n",
      "date                       \n",
      "2017-04-01         0.019920\n",
      "2017-04-04         0.017391\n",
      "2017-04-05         0.234615\n",
      "2017-04-06         0.294574\n",
      "2017-04-07         0.606870\n",
      "2017-04-08         0.593074\n",
      "2017-04-09         0.283105\n",
      "2017-04-10         0.207048\n",
      "\n",
      "Average percent of newspaper front page devoted to Syria by day:\n",
      "date\n",
      "2017-04-01    0.000628\n",
      "2017-04-04    0.000541\n",
      "2017-04-05    0.005393\n",
      "2017-04-06    0.008891\n",
      "2017-04-07    0.028889\n",
      "2017-04-08    0.031970\n",
      "2017-04-09    0.010762\n",
      "2017-04-10    0.010199\n",
      "Name: percent_of_page, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('''Percent of papers that mentioned Syria by day:\n",
    "{}\n",
    "\n",
    "Average percent of newspaper front page devoted to Syria by day:\n",
    "{}'''.format(\n",
    "    percent_of_papers_with_mention(syria_results),\n",
    "    percent_of_page_by_day(syria_results),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting newspapers with population metadata\n",
    "\n",
    "Short of getting data on readership, we'll try to pull population metadata for the hometown of each newspaper.\n",
    "\n",
    "Edit: See bottom for conclusion. Tldr: it's not great, because there are multiple papers per city, many of which are lesser read. Doh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_population = pd.read_csv('~/data/sub-est2015_all.csv', encoding='ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUMLEV</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>PLACE</th>\n",
       "      <th>COUSUB</th>\n",
       "      <th>CONCIT</th>\n",
       "      <th>PRIMGEO_FLAG</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>NAME</th>\n",
       "      <th>STNAME</th>\n",
       "      <th>CENSUS2010POP</th>\n",
       "      <th>ESTIMATESBASE2010</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>city</th>\n",
       "      <th>place_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35950</th>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>39878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Mankato city</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mankato</td>\n",
       "      <td>mankato, minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40886</th>\n",
       "      <td>157</td>\n",
       "      <td>29</td>\n",
       "      <td>137</td>\n",
       "      <td>28000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>Goss town</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>goss</td>\n",
       "      <td>goss, missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81136</th>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Milwaukee city</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>milwaukee</td>\n",
       "      <td>milwaukee, wisconsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44352</th>\n",
       "      <td>61</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>42820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>Livermore town</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>livermore</td>\n",
       "      <td>livermore, new hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23257</th>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>7975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Bonner Springs city</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bonner springs</td>\n",
       "      <td>bonner springs, kansas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SUMLEV  STATE  COUNTY  PLACE  COUSUB  CONCIT  PRIMGEO_FLAG FUNCSTAT  \\\n",
       "35950      61     27     103      0   39878       0             0        F   \n",
       "40886     157     29     137  28000       0       0             1        A   \n",
       "81136      61     55     133      0   53000       0             0        F   \n",
       "44352      61     33       9      0   42820       0             1        I   \n",
       "23257      61     20      91      0    7975       0             0        F   \n",
       "\n",
       "                      NAME         STNAME CENSUS2010POP  ESTIMATESBASE2010  \\\n",
       "35950         Mankato city      Minnesota             0                  0   \n",
       "40886            Goss town       Missouri             0                  0   \n",
       "81136       Milwaukee city      Wisconsin             0                  0   \n",
       "44352       Livermore town  New Hampshire             0                  0   \n",
       "23257  Bonner Springs city         Kansas             0                  0   \n",
       "\n",
       "       POPESTIMATE2010  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  \\\n",
       "35950                0                0                0                0   \n",
       "40886                0                0                0                0   \n",
       "81136                0                0                0                0   \n",
       "44352                0                0                0                0   \n",
       "23257                0                0                0                0   \n",
       "\n",
       "       POPESTIMATE2014  POPESTIMATE2015            city  \\\n",
       "35950                0                0         mankato   \n",
       "40886                0                0            goss   \n",
       "81136                0                0       milwaukee   \n",
       "44352                0                0       livermore   \n",
       "23257                0                0  bonner springs   \n",
       "\n",
       "                     place_name  \n",
       "35950        mankato, minnesota  \n",
       "40886            goss, missouri  \n",
       "81136      milwaukee, wisconsin  \n",
       "44352  livermore, new hampshire  \n",
       "23257    bonner springs, kansas  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities = df_population[df_population.NAME.str.endswith('city') | df_population.NAME.str.endswith('town')]\n",
    "df_cities['city'] = df_cities.NAME.str.slice(0, -5).str.lower()\n",
    "df_cities['place_name'] = df_cities.city + ', ' + df_cities.STNAME.str.lower()\n",
    "\n",
    "df_cities = df_cities.sort_values('POPESTIMATE2015').groupby('place_name').head(1)\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_abbreviation_to_name = {}\n",
    "with open('files/states.csv') as f:\n",
    "    next(f) # skip header\n",
    "    for line in f:\n",
    "        state, abbrev = line.strip().split(',')\n",
    "        state_abbreviation_to_name[abbrev.strip('\"')] = state.strip('\"').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "us_newspapers_df['place_name'] = us_newspapers_df.city.str.lower() + ', ' + us_newspapers_df.state.apply(state_abbreviation_to_name.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_newspapers_with_pop = pd.merge(us_newspapers_df, df_cities[['place_name', 'POPESTIMATE2015']], how='left', on='place_name', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614 out of 685 newspapers had places found in the census.\n",
      "\n",
      "Examples of ones that didn't:\n",
      "6          anchorage, alaska\n",
      "8             juneau, alaska\n",
      "45       ventura, california\n",
      "52      cañon city, colorado\n",
      "104    the villages, florida\n",
      "Name: place_name, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''{} out of {} newspapers had places found in the census.\n",
    "\n",
    "Examples of ones that didn't:\n",
    "{}\n",
    "'''.format(\n",
    "    us_newspapers_with_pop.POPESTIMATE2015.count(),\n",
    "    us_newspapers_with_pop.shape[0],\n",
    "    us_newspapers_with_pop[us_newspapers_with_pop.POPESTIMATE2015.isnull()].place_name.head()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "      <th>place_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Centre</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.152336</td>\n",
       "      <td>-85.678963</td>\n",
       "      <td>AL_TP</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Post</td>\n",
       "      <td>http://www.epostpaper.com</td>\n",
       "      <td>centre, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decatur</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.602890</td>\n",
       "      <td>-86.986511</td>\n",
       "      <td>AL_DD</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Decatur Daily</td>\n",
       "      <td>http://www.decaturdaily.com</td>\n",
       "      <td>decatur, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dothan</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.225517</td>\n",
       "      <td>-85.393631</td>\n",
       "      <td>AL_DE</td>\n",
       "      <td>AL</td>\n",
       "      <td>Dothan Eagle</td>\n",
       "      <td>http://www.dothaneagle.com</td>\n",
       "      <td>dothan, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Florence</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.799538</td>\n",
       "      <td>-87.677467</td>\n",
       "      <td>AL_TD</td>\n",
       "      <td>AL</td>\n",
       "      <td>TimesDaily</td>\n",
       "      <td>http://www.timesdaily.com</td>\n",
       "      <td>florence, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Montgomery</td>\n",
       "      <td>USA</td>\n",
       "      <td>32.379051</td>\n",
       "      <td>-86.314224</td>\n",
       "      <td>AL_MA</td>\n",
       "      <td>AL</td>\n",
       "      <td>Montgomery Advertiser</td>\n",
       "      <td>http://www.montgomeryadvertiser.com</td>\n",
       "      <td>montgomery, alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city country   latitude  longitude   slug state  \\\n",
       "0      Centre     USA  34.152336 -85.678963  AL_TP    AL   \n",
       "1     Decatur     USA  34.602890 -86.986511  AL_DD    AL   \n",
       "2      Dothan     USA  31.225517 -85.393631  AL_DE    AL   \n",
       "3    Florence     USA  34.799538 -87.677467  AL_TD    AL   \n",
       "4  Montgomery     USA  32.379051 -86.314224  AL_MA    AL   \n",
       "\n",
       "                   title                              website  \\\n",
       "0               The Post            http://www.epostpaper.com   \n",
       "1      The Decatur Daily          http://www.decaturdaily.com   \n",
       "2           Dothan Eagle           http://www.dothaneagle.com   \n",
       "3             TimesDaily            http://www.timesdaily.com   \n",
       "4  Montgomery Advertiser  http://www.montgomeryadvertiser.com   \n",
       "\n",
       "            place_name  \n",
       "0      centre, alabama  \n",
       "1     decatur, alabama  \n",
       "2      dothan, alabama  \n",
       "3    florence, alabama  \n",
       "4  montgomery, alabama  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_newspapers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidentified_map = {}\n",
    "\n",
    "unidentified_places = us_newspapers_with_pop[us_newspapers_with_pop.POPESTIMATE2015.isnull()]\n",
    "\n",
    "for i, row in unidentified_places.iterrows():\n",
    "    matches = (df_population.STNAME == row.state) & (df_population.NAME.str.lower().str.contains(row.city.lower()))\n",
    "    if matches.sum() == 0:\n",
    "        continue\n",
    "        \n",
    "    pops = df_population[matches].sort_values('POPESTIMATE2015').iloc[0]\n",
    "    unidentified_map[row.place_name] = (pops.NAME, pops.POPESTIMATE2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 71 unidentified places, we found 0 by looking for substrings.\n"
     ]
    }
   ],
   "source": [
    "print('''Out of {} unidentified places, we found {} by looking for substrings.'''.format(\n",
    "    unidentified_places.shape[0],\n",
    "    len(unidentified_map)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So now 614 out of 685 newspapers have populations.\n",
      "\n",
      "Largest newspapers by population:\n",
      "                                         title state\n",
      "292                                AM New York    NY\n",
      "297                         The New York Times    NY\n",
      "294  The Epoch Times: Chinese New York Edition    NY\n",
      "298                    The Wall Street Journal    NY\n",
      "673                         Impacto Latin News    NY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_from_map_if_null(row):\n",
    "    if pd.isnull(row.POPESTIMATE2015):\n",
    "        return unidentified_map.get(row.place_name, [np.nan, np.nan])[1]\n",
    "    \n",
    "    return row.POPESTIMATE2015\n",
    "\n",
    "us_newspapers_with_pop['population_est_2015'] = us_newspapers_with_pop.apply(set_from_map_if_null, 1)\n",
    "\n",
    "print('''So now {} out of {} newspapers have populations.\n",
    "\n",
    "Largest newspapers by population:\n",
    "{}\n",
    "'''.format(\n",
    "    us_newspapers_with_pop.population_est_2015.count(),\n",
    "    us_newspapers_with_pop.shape[0],\n",
    "    us_newspapers_with_pop.sort_values('population_est_2015', ascending=False).head(5)[['title', 'state']]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof. Looks like population might not work so well, since large cities often have several, lesser-read newspapers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most headline-y words per day\n",
    "\n",
    "This is a variation on the unigram experiment above, where instead we will compute the percent of page for _all_ words in all newspapers. Then we'll average them together across the newspapers to get the \"most headliney words\".\n",
    "\n",
    "A few variations we'll consider:\n",
    "\n",
    "* We'll run one version where we consider the area given to each word independently, and another one where the bounding box of the entire text box where the word is found is grouped together. In terms of the front page real estate, one approach can be viewed as basically calculating the real estate for individual words, and the other for \"topics\" where topics consist of all the words in the document.\n",
    "* There is going to be a lot of noise from stopwords. \"The\", for instance, will be present in nearly all of the articles. We should perform tf-idf to scale the data first. However, we don't want tf-idf to count newsy words toward the document frequency, so we'll calculate it on a separate corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1624.6550618037304),\n",
       " ('slope', 1448.3909144144402),\n",
       " ('faces', 1401.443451),\n",
       " ('threat', 1401.443451),\n",
       " ('button', 1071.1280708711313)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, without any idf weighting, we'll calculate the contribution of individual words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def vocab_weights_by_word(df):\n",
    "    counter = Counter()\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        for word in row.bow:\n",
    "            # we won't multiply by the number of characters to get closer to \"true\" word real estate because we don't\n",
    "            # care about the length of words. but we will divide by the total area of the page to normalize across\n",
    "            # newspapers that are different sizes.\n",
    "            counter[word] += row.avg_character_area \n",
    "    \n",
    "    return counter\n",
    "\n",
    "sorted(vocab_weights_by_word(df_denver_post_latest).items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there needs to be some kind of weighting, or else words like \"by\" will dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll calculate document frequencies across the 10788 articles in the Reuters corpus.\n",
      "\n",
      "The most common words in the corpus are:\n",
      "[('of', 7622), ('the', 6951), ('to', 6944), ('said', 6784), ('and', 6765)]\n",
      "\n",
      "As idfs:\n",
      "[('of', 0.34739560282581505), ('the', 0.43954887115913471), ('to', 0.4405564279194345), ('said', 0.46386750678788152), ('and', 0.4666721436547322)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "doc_freq_counter = Counter()\n",
    "\n",
    "for fid in reuters.fileids():\n",
    "    bow = set(map(operator.methodcaller('lower'), reuters.words(fid)))\n",
    "    bow = bow - set(string.punctuation) - set(string.digits)\n",
    "    doc_freq_counter.update(bow)\n",
    "\n",
    "idfs = {}\n",
    "for word, count in doc_freq_counter.items():\n",
    "    idfs[word] = np.log(float(len(reuters.fileids())) / count)\n",
    "    \n",
    "print('''We'll calculate document frequencies across the {} articles in the Reuters corpus.\n",
    "\n",
    "The most common words in the corpus are:\n",
    "{}\n",
    "\n",
    "As idfs:\n",
    "{}\n",
    "'''.format(\n",
    "    len(reuters.fileids()),\n",
    "    sorted(doc_freq_counter.items(), key=operator.itemgetter(1), reverse=True)[:5],\n",
    "    sorted(idfs.items(), key=operator.itemgetter(1))[:5],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words in the latest Denver Post by aggregate word \"real estate\",\n",
      "weighted by inverse document frequency in the Reuter's corpus:\n",
      "[('slope', 10631.59418805217),\n",
      " ('button', 9946.6984422444057),\n",
      " ('who’s', 9261.5177454864843),\n",
      " ('faces', 7710.7419379525691),\n",
      " ('threat', 7276.077344885779),\n",
      " ('got', 5443.048444262532),\n",
      " ('ewest', 5067.919523421433),\n",
      " ('nuggets', 4435.4022027479232),\n",
      " ('thunder', 4435.4022027479232),\n",
      " ('consulate', 3448.3858287424287)]\n",
      "\n",
      "With word areas taken into consideration (longer words get weighted higher):\n",
      "[('button', 59680.190653466438),\n",
      " ('slope', 53157.970940260842),\n",
      " ('who’s', 46307.58872743242),\n",
      " ('threat', 43656.464069314672),\n",
      " ('faces', 38553.709689762843),\n",
      " ('applewoodfixi', 37804.285008065148),\n",
      " ('nuggets', 31047.815419235463),\n",
      " ('thunder', 31047.815419235463),\n",
      " ('consulate', 31035.472458681856),\n",
      " ('eliminated', 30290.373127232895)]\n",
      "\n",
      "Using the area of the entire block:\n",
      "[('colorado', 0.75583500353502198),\n",
      " ('society', 0.69532509894094863),\n",
      " ('anthem', 0.68897999771696461),\n",
      " ('slope', 0.6389571971868433),\n",
      " ('affordable', 0.62194197789964389),\n",
      " ('state’s', 0.62194197789964389),\n",
      " ('junction', 0.62194197789964389),\n",
      " ('gov', 0.62194197789964389),\n",
      " ('two-thirds', 0.62194197789964389),\n",
      " ('57year-old', 0.62194197789964389)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again, this time with idf weighting\n",
    "\n",
    "def vocab_weights_by_word(df, idf=None, method='by_char'):\n",
    "    '''Methods:\n",
    "    `by_char`: Average character size of the textbox in which a string is embedded\n",
    "    `by_word_area`: Average character size * len of string\n",
    "    `by_block`: Area of block in which string is embedded'''\n",
    "    if method not in ['by_char', 'by_word_area', 'by_block']:\n",
    "        raise ArgumentError('method needs to be one of \"by_char\", \"by_word_area\", \"by_block\"')\n",
    "        \n",
    "    counter = Counter()\n",
    "    \n",
    "    max_idf = max(idf.values()) # used for missing values\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        for word in set(row.bow) - set(string.punctuation) - set(string.digits):\n",
    "            # we won't multiply by the number of characters to get closer to \"true\" word real estate because we don't\n",
    "            # care about the length of words. but we will divide by the total area of the page to normalize across\n",
    "            # newspapers that are different sizes.\n",
    "            \n",
    "            if method in ['by_char', 'by_word_area']:\n",
    "                weight = row.avg_character_area\n",
    "\n",
    "                if method == 'by_word_area':\n",
    "                    weight *= len(word)\n",
    "            elif method == 'by_block':\n",
    "                weight = row.percent_of_page\n",
    "            \n",
    "            if idf:\n",
    "                weight *= idf.get(word, max_idf)\n",
    "                \n",
    "            counter[word] += weight\n",
    "            \n",
    "    \n",
    "    return counter\n",
    "\n",
    "print('''The top words in the latest Denver Post by aggregate word \"real estate\",\n",
    "weighted by inverse document frequency in the Reuter's corpus:\n",
    "{}\n",
    "\n",
    "With word areas taken into consideration (longer words get weighted higher):\n",
    "{}\n",
    "\n",
    "Using the area of the entire block:\n",
    "{}\n",
    "\n",
    "'''.format(\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs).items(), key=operator.itemgetter(1), reverse=True)[:10]),\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs, method='by_word_area').items(), key=operator.itemgetter(1), reverse=True)[:10]),\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs, method='by_block').items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better document frequencies\n",
    "\n",
    "The Reuters corpus is only ~10k documents. Instead, let's reverse engineer the document frequencies from the words in a word2vec model of Google News and Zipf's Law.\n",
    "\n",
    "(Skip to other window, where I did this, and found the results to be lackluster.)\n",
    "\n",
    "I requested access to the Yahoo News n-grams corpus. Otherwise, may need to be creative.\n",
    "\n",
    "For now, let's incorporate the document frequencies from the articles themselves in the dataset. The more days we gather, the more we'll be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_word_doc_counts = Counter()\n",
    "\n",
    "for i, row in df_us.iterrows():\n",
    "    article_word_doc_counts.update(row.bow)\n",
    "\n",
    "article_idfs = {}\n",
    "for word, count in article_word_doc_counts.items():\n",
    "    article_idfs[word] = np.log(float(df_us.shape[0]) / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of these two different idf datasets:\n",
      "\n",
      "Reuters: 31046\n",
      "Front pages: 76600\n",
      "\n",
      "Most common front page words:\n",
      "[('the', 1.440024260449646),\n",
      " ('and', 1.7965947459366325),\n",
      " ('for', 2.0606261497212492),\n",
      " ('that', 2.5732908654466566),\n",
      " ('with', 2.6091893481244282),\n",
      " ('page', 2.7279795814548811),\n",
      " ('see', 2.7348399513786625),\n",
      " ('said', 2.7445809756749249),\n",
      " ('from', 2.795359212069743),\n",
      " ('was', 2.8183070789577749)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''Vocabulary size of these two different idf datasets:\n",
    "\n",
    "Reuters: {}\n",
    "Front pages: {}\n",
    "\n",
    "Most common front page words:\n",
    "{}\n",
    "'''.format(\n",
    "    len(idfs),\n",
    "    len(article_idfs),\n",
    "    pprint.pformat(sorted(article_idfs.items(), key=operator.itemgetter(1))[:10])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding \"front-page-est\" words\n",
    "\n",
    "By combining the results of running all of the newspapers on a given day through the method above, we attempt to find the words most representative of front pages across the country on any particular day.\n",
    "\n",
    "We'll run it using all three of the different methods we have for weighting words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  227\n",
      ".....Top results with word area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('garcia', 5614.4030407721866),\n",
       " ('the', 5104.4952049282119),\n",
       " ('advocate', 5010.8674427767937),\n",
       " ('masters', 4963.7040106961076),\n",
       " ('egypt', 4279.5201922270999),\n",
       " ('for', 4159.8522602366793),\n",
       " ('sunday', 4057.2769053329234),\n",
       " ('enquirer', 4039.9825467214009),\n",
       " ('daily', 3949.6023652686245),\n",
       " ('churches', 3757.0229306111673)]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_word_area')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with word area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  227\n",
      ".....Top results with character area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1701.4984016427366),\n",
       " ('for', 1386.6174200788939),\n",
       " ('and', 942.95278424218009),\n",
       " ('garcia', 935.73384012869826),\n",
       " ('with', 928.61987534276875),\n",
       " ('egypt', 855.90403844541981),\n",
       " ('trg', 793.54828277428692),\n",
       " ('daily', 789.92047305372557),\n",
       " ('masters', 709.10057295658737),\n",
       " ('times', 695.14749962830138)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_char')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with character area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  227\n",
      ".....Top results with block area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('and', 0.29765168003819514),\n",
       " ('the', 0.2817685853603083),\n",
       " ('for', 0.27035423371818018),\n",
       " ('that', 0.25301834547770419),\n",
       " ('said', 0.24027583877802652),\n",
       " ('with', 0.2309388806449186),\n",
       " ('from', 0.21287681935860919),\n",
       " ('but', 0.20830897879846408),\n",
       " ('was', 0.20548308749193064),\n",
       " ('are', 0.19901211017601411)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_block')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with block area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! So it looks like:\n",
    "\n",
    "* all the methods give too much weight to frequency of appearance, vs. rareness of word\n",
    "* this is especially evident with block area, since every time \"the\" shows up, the entire area of the block is counted\n",
    "* the word area weight gives some interesting results but they get skewed by extremely large banners (the enquirer, the advocate, \"sunday\", etc)\n",
    "\n",
    "So that means the next steps are:\n",
    "* remove stopwords\n",
    "* remove names of newspapers from themselves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
