{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front pages of newspapers -- Initial discovery\n",
    "\n",
    "We have two datasets:\n",
    "* `frontpage_texts`, the text boxes extracted from pdfs of the front pages of newspapers, downloaded from the [Newseum](https://newseum.org/todaysfrontpages/)\n",
    "* `newspapers`, the metadata of the newspapers, also from the Newseum site.\n",
    "\n",
    "The text boxes contain interesting metadata for a given chunk of text, such as its bounding box, font, and size.\n",
    "\n",
    "This notebook will document some of the early exploratory attempts to understand the variety of the data, and to move toward performing an analysis of media coverage/bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <help>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(clean=True, us=True):\n",
    "    df = pd.read_sql_table('frontpage_texts', 'postgres:///frontpages')\n",
    "    \n",
    "    df_newspapers = pd.read_sql_table('newspapers', 'postgres:///frontpages')\n",
    "    \n",
    "    if clean:\n",
    "        df['text'] = df['text'].str.strip()\n",
    "        df = df[df['text'].str.len() > 1]\n",
    "        \n",
    "        # This is the date that the Newseum had a \"Day without News\":\n",
    "        # http://www.newseum.org/withoutnews/\n",
    "        df = df[df.date != datetime.datetime(2017, 6, 5)]\n",
    "\n",
    "        df = dedupe_text(df)\n",
    "    \n",
    "    if us:\n",
    "        df_newspapers = df_newspapers[df_newspapers.country == 'USA']\n",
    "        df = df[df.slug.isin(set(df_newspapers.slug))]\n",
    "        \n",
    "    \n",
    "    df['page_height_round'] = df['page_height'].apply(int)\n",
    "    df['page_width_round'] = df['page_width'].apply(int)\n",
    "    df['page_width_round_10'] = df['page_width'].apply(lambda w: int(w/10)*10)\n",
    "    df['page_height_round_10'] = df['page_height'].apply(lambda w: int(w/10)*10)\n",
    "    df['aspect_ratio'] = np.round(df['page_width_round_10'] / df['page_height_round_10'], decimals=1) \n",
    "\n",
    "    return df, df_newspapers\n",
    "\n",
    "def dedupe_text(df):\n",
    "    text_counts = df.groupby(['slug']).text.value_counts()\n",
    "    duplicate_text = text_counts[text_counts > 1].reset_index(name='count').drop('count', axis=1)\n",
    "\n",
    "    duplicate_text_dict = defaultdict(set)\n",
    "    duplicate_text.apply(lambda row: duplicate_text_dict[row.slug].add(row.text), axis=1)\n",
    "\n",
    "    return df[df.apply(lambda row: row.text not in duplicate_text_dict[row.slug], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df, df_newspapers = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean = dedupe_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anniston</td>\n",
       "      <td>USA</td>\n",
       "      <td>33.696739</td>\n",
       "      <td>-85.823433</td>\n",
       "      <td>AL_AS</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Anniston Star</td>\n",
       "      <td>http://www.annistonstar.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>USA</td>\n",
       "      <td>33.518509</td>\n",
       "      <td>-86.804756</td>\n",
       "      <td>AL_BN</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Birmingham News</td>\n",
       "      <td>http://www.al.com/birmingham/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cullman</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.176857</td>\n",
       "      <td>-86.838188</td>\n",
       "      <td>AL_CT</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Cullman Times</td>\n",
       "      <td>http://www.cullmantimes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decatur</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.602890</td>\n",
       "      <td>-86.986511</td>\n",
       "      <td>AL_DD</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Decatur Daily</td>\n",
       "      <td>http://www.decaturdaily.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dothan</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.225517</td>\n",
       "      <td>-85.393631</td>\n",
       "      <td>AL_DE</td>\n",
       "      <td>AL</td>\n",
       "      <td>Dothan Eagle</td>\n",
       "      <td>http://www.dothaneagle.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city country   latitude  longitude   slug state                title  \\\n",
       "0    Anniston     USA  33.696739 -85.823433  AL_AS    AL    The Anniston Star   \n",
       "1  Birmingham     USA  33.518509 -86.804756  AL_BN    AL  The Birmingham News   \n",
       "2     Cullman     USA  34.176857 -86.838188  AL_CT    AL    The Cullman Times   \n",
       "3     Decatur     USA  34.602890 -86.986511  AL_DD    AL    The Decatur Daily   \n",
       "4      Dothan     USA  31.225517 -85.393631  AL_DE    AL         Dothan Eagle   \n",
       "\n",
       "                         website  \n",
       "0    http://www.annistonstar.com  \n",
       "1  http://www.al.com/birmingham/  \n",
       "2    http://www.cullmantimes.com  \n",
       "3    http://www.decaturdaily.com  \n",
       "4     http://www.dothaneagle.com  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_newspapers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have metadata for 1082 newspapers.\n",
      "\n",
      "There are 80 total countries represented. The top 5 are:\n",
      "USA       731\n",
      "Brazil     60\n",
      "Canada     41\n",
      "India      21\n",
      "Mexico     20\n",
      "Name: country, dtype: int64.\n",
      "\n",
      "Within the US, there is representation from 51 states. The states with the most newspapers are:\n",
      "NY    60\n",
      "CA    53\n",
      "PA    44\n",
      "FL    33\n",
      "NC    29\n",
      "Name: state, dtype: int64\n",
      "\n",
      "And the least:\n",
      "NV    3\n",
      "RI    3\n",
      "AK    3\n",
      "DE    2\n",
      "HI    1\n",
      "Name: state, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "us_newspapers_df = df_newspapers[df_newspapers.country == 'USA']\n",
    "print('''We have metadata for {} newspapers.\n",
    "\n",
    "There are {} total countries represented. The top 5 are:\n",
    "{}.\n",
    "\n",
    "Within the US, there is representation from {} states. The states with the most newspapers are:\n",
    "{}\n",
    "\n",
    "And the least:\n",
    "{}\n",
    "\n",
    "'''.format(\n",
    "    df_newspapers.shape[0],\n",
    "    df_newspapers.country.nunique(),\n",
    "    df_newspapers.country.value_counts()[:5],\n",
    "    us_newspapers_df.state.nunique(),\n",
    "    us_newspapers_df.state.value_counts()[:5],\n",
    "    us_newspapers_df.state.value_counts()[-5:],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently, there are:\n",
      "\n",
      "268442 rows of text\n",
      "14 days of scrapes\n",
      "  (earliest: 2017-04-01 00:00:00 \n",
      "   latest  : 2017-04-16 00:00:00)\n",
      "601 total newspapers (not all the pdfs were extractable).\n",
      "\n",
      "Filtering down to the US, there are now:\n",
      "417 newspapers\n",
      "193969 rows of text\n",
      "\n",
      "For those newspapers that are available in the US, there are:\n",
      "50 states\n",
      "states with most newspapers:\n",
      "CA    25\n",
      "PA    24\n",
      "IN    19\n",
      "TX    19\n",
      "FL    18\n",
      "Name: state, dtype: int64\n",
      "\n",
      "with least:\n",
      "DE    2\n",
      "NE    2\n",
      "RI    2\n",
      "NV    1\n",
      "NH    1\n",
      "Name: state, dtype: int64\n",
      "\n",
      "with none:\n",
      "{'', 'HI'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us = df[df.slug.isin(set(us_newspapers_df.slug))]\n",
    "newspapers_in_df = df_newspapers[df_newspapers.slug.isin(set(df_us.slug))]\n",
    "\n",
    "print('''Currently, there are:\n",
    "\n",
    "{} rows of text\n",
    "{} days of scrapes\n",
    "  (earliest: {} \n",
    "   latest  : {})\n",
    "{} total newspapers (not all the pdfs were extractable).\n",
    "\n",
    "Filtering down to the US, there are now:\n",
    "{} newspapers\n",
    "{} rows of text\n",
    "\n",
    "For those newspapers that are available in the US, there are:\n",
    "{} states\n",
    "states with most newspapers:\n",
    "{}\n",
    "\n",
    "with least:\n",
    "{}\n",
    "\n",
    "with none:\n",
    "{}\n",
    "'''.format(\n",
    "    df.shape[0],\n",
    "    df.date.nunique(),\n",
    "    df.date.min(),\n",
    "    df.date.max(),\n",
    "    df.slug.nunique(),\n",
    "\n",
    "    df_us.slug.nunique(),\n",
    "    df_us.shape[0],\n",
    "    \n",
    "    newspapers_in_df.state.nunique(),\n",
    "    newspapers_in_df.state.value_counts()[:5],\n",
    "    newspapers_in_df.state.value_counts()[-5:],\n",
    "    set(df_newspapers.state) - set(newspapers_in_df.state)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonts are often written in a format like this: HHDPCE+HelveticaNeueLTStd-BdCn.\n",
      "\n",
      "Out of 268442 rows...\n",
      "268442 of the fonts have non-empty text\n",
      "186893 of the fonts have a '+'\n",
      "233542 of the fonts have a '-'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''Fonts are often written in a format like this: {}.\n",
    "\n",
    "Out of {} rows...\n",
    "{} of the fonts have non-empty text\n",
    "{} of the fonts have a '+'\n",
    "{} of the fonts have a '-'\n",
    "'''.format(\n",
    "    df.fontface.iloc[0],\n",
    "    df.shape[0],\n",
    "    (df.fontface.str.len() > 0).sum(),\n",
    "    df.fontface.str.contains('\\+').sum(),\n",
    "    df.fontface.str.contains('-').sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This seems to mean that we can break apart the font into:\n",
      "[optional-leading-thing]+[font-family]-[font-weight]\n",
      "\n",
      "After doing that,\n",
      "There are...\n",
      "2101 unique font families\n",
      "1833 unique font weights\n",
      "32906 unique optional-leading-things\n"
     ]
    }
   ],
   "source": [
    "print('''This seems to mean that we can break apart the font into:\n",
    "[optional-leading-thing]+[font-family]-[font-weight]\n",
    "''')\n",
    "\n",
    "font_partition = df.fontface.str.rpartition('+')\n",
    "df['font_family_weight'] = font_partition[2]\n",
    "\n",
    "font_family_partition = df['font_family_weight'].str.partition('-')\n",
    "\n",
    "df['font_leading_thing'] = font_partition[0]\n",
    "df['font_family'] = font_family_partition[0]\n",
    "df['font_weight'] = font_family_partition[2]\n",
    "\n",
    "print('''After doing that,\n",
    "There are...\n",
    "{} unique font families\n",
    "{} unique font weights\n",
    "{} unique optional-leading-things'''.format(\n",
    "    df.font_family.nunique(),\n",
    "    df.font_weight.nunique(),\n",
    "    df.font_leading_thing.nunique()\n",
    "))\n",
    "\n",
    "df_us = df[df.slug.isin(set(us_newspapers_df.slug))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denver Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Denver</td>\n",
       "      <td>USA</td>\n",
       "      <td>39.741684</td>\n",
       "      <td>-104.987366</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>CO</td>\n",
       "      <td>The Denver Post</td>\n",
       "      <td>http://www.denverpost.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city country   latitude   longitude   slug state            title  \\\n",
       "67  Denver     USA  39.741684 -104.987366  CO_DP    CO  The Denver Post   \n",
       "\n",
       "                      website  \n",
       "67  http://www.denverpost.com  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do something with a Denver paper\n",
    "\n",
    "df_newspapers[df_newspapers.city == 'Denver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 12 days of scraped Denver Post front pages.\n",
      "\n",
      "We have 53 unique font-weight combos. Here is a mapping of each font family to their min, average, and max font size.\n",
      "\n",
      "                                count     min      max        avg\n",
      "font_family_weight                                               \n",
      "Agenda-Bold                       2.0  31.870   31.870  31.870000\n",
      "AkzidenzGroteskBE-LightEx         1.0   4.354    4.354   4.354000\n",
      "AkzidenzGroteskBE-MdEx            2.0   6.244   13.537   9.890500\n",
      "Arial-Black                       2.0  26.410   26.410  26.410000\n",
      "Arial-BoldItalicMT                2.0   9.469    9.469   9.469000\n",
      "Avenir-Heavy                      6.0   4.853    4.853   4.853000\n",
      "AzoSansUber-Regular               2.0  19.680   24.928  22.304000\n",
      "CastleT-Ultr                      2.0  39.039   39.039  39.039000\n",
      "DPPiFont                          4.0   8.504    8.504   8.504000\n",
      "Gotham-Black                      3.0  11.359   17.680  14.132667\n",
      "Gotham-Bold                       4.0   6.630   19.570  10.751750\n",
      "Gotham-Medium                     3.0  14.131   23.751  19.330333\n",
      "Gotham-Thin                       1.0   8.839    8.839   8.839000\n",
      "GothamBold                        6.0   6.674   21.371  14.871500\n",
      "MyriadPro-Bold                    2.0  27.258   39.648  33.453000\n",
      "MyriadPro-BoldIt                  4.0  17.318   27.845  22.674250\n",
      "MyriadPro-It                      2.0  13.233   13.534  13.383500\n",
      "MyriadPro-Regular                22.0   9.616   10.097  10.053273\n",
      "MyriadPro-SemiboldIt              2.0  13.442   18.330  15.886000\n",
      "NewBaskervilleStd-Bold            1.0  15.314   15.314  15.314000\n",
      "NewBaskervilleStd-Italic          1.0  15.340   15.340  15.340000\n",
      "NewBaskervilleStd-Roman           1.0  12.958   12.958  12.958000\n",
      "PoynterOSDisplay-Bold            11.0  12.551   22.820  15.963636\n",
      "PoynterOSDisplay-Italic          12.0  16.605   16.605  16.605000\n",
      "PoynterOSDisplay-Roman           13.0  16.740   89.280  46.073615\n",
      "PoynterOSDisplay-Semibold         8.0  15.778   40.572  26.766250\n",
      "PoynterOSDisplay-SemiboldItal     2.0  19.176   20.304  19.740000\n",
      "PoynterOSDisplayNarrow-Bold      15.0  24.904  104.144  60.448800\n",
      "PoynterOSDisplayNarrow-Semibld   12.0  23.366   41.366  32.133167\n",
      "PoynterOSTextThree-Bold          19.0  10.214   10.822  10.246000\n",
      "PoynterOSTextThree-Italic        19.0   8.344   11.920   9.410632\n",
      "PoynterOSTextThree-Roman         80.0  10.013   13.112  10.789275\n",
      "PoynterOSTextThree-SemiBld       14.0  10.671   10.671  10.671000\n",
      "PoynterOSTextTwoL-Italic          3.0   8.302    8.302   8.302000\n",
      "Sun-Bold                         33.0  10.860   11.946  11.518182\n",
      "Sun-ExtraBold                    16.0   7.973   13.668   9.966000\n",
      "Sun-Regular                       2.0   8.963    9.496   9.229500\n",
      "Sun-SemiBold                     50.0  10.114   36.584  18.748080\n",
      "SunSC-Bold                       10.0   9.514    9.514   9.514000\n",
      "SunSC-ExtraBold                  34.0   8.680    8.680   8.680000\n",
      "SunSC-Light                       1.0  11.385   11.385  11.385000\n",
      "SunSC-Regular                    16.0   7.322   10.460   7.714250\n",
      "SunSC-SemiBold                    2.0   9.936   16.912  13.424000\n",
      "TheSerifBold-Caps                 4.0  14.454   14.454  14.454000\n",
      "TheSerifExtraBold-Plain           6.0  18.007   29.466  26.737667\n",
      "TheSerifLight-Plain              26.0  22.232   22.232  22.232000\n",
      "TradeGothicLTStd-Cn18             2.0   2.664    8.162   5.413000\n",
      "Univers-Bold                      4.0  11.880   38.265  25.072500\n",
      "Univers-Condensed                 2.0  18.585   22.302  20.443500\n",
      "Univers-CondensedBold             1.0  32.913   32.913  32.913000\n",
      "UniversLTStd                      1.0  10.773   10.773  10.773000\n",
      "UniversLTStd-Bold                 1.0  22.572   22.572  22.572000\n",
      "Veneer                            3.0  43.472   71.978  52.974000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_denver_post = df_us[df_us.slug == 'CO_DP']\n",
    "\n",
    "font_stats = df_denver_post.groupby(['font_family_weight']).fontsize.agg({'count': len, 'min': np.min, 'max': np.max, 'avg': np.mean})\n",
    "\n",
    "print('''We have {} days of scraped Denver Post front pages.\n",
    "\n",
    "We have {} unique font-weight combos. Here is a mapping of each font family to their min, average, and max font size.\n",
    "\n",
    "{}\n",
    "'''.format(\n",
    "    df_denver_post.date.nunique(),\n",
    "    df_denver_post.groupby(['font_family_weight']).first().shape[0],\n",
    "    font_stats\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonts by number of days on which they appear\n",
      "\n",
      "font_family_weight\n",
      "Sun-SemiBold                      12\n",
      "SunSC-ExtraBold                   12\n",
      "PoynterOSDisplay-Italic           12\n",
      "PoynterOSDisplayNarrow-Bold       12\n",
      "PoynterOSTextThree-Roman          12\n",
      "SunSC-Regular                     12\n",
      "Sun-Bold                          12\n",
      "TheSerifLight-Plain               11\n",
      "PoynterOSDisplayNarrow-Semibld    10\n",
      "PoynterOSTextThree-Bold           10\n",
      "PoynterOSTextThree-Italic         10\n",
      "PoynterOSTextThree-SemiBld        10\n",
      "PoynterOSDisplay-Roman             9\n",
      "PoynterOSDisplay-Bold              9\n",
      "SunSC-Bold                         7\n",
      "PoynterOSDisplay-Semibold          7\n",
      "TheSerifExtraBold-Plain            5\n",
      "Sun-ExtraBold                      4\n",
      "DPPiFont                           4\n",
      "TheSerifBold-Caps                  3\n",
      "MyriadPro-Regular                  3\n",
      "MyriadPro-Bold                     2\n",
      "MyriadPro-It                       2\n",
      "MyriadPro-SemiboldIt               2\n",
      "CastleT-Ultr                       2\n",
      "AzoSansUber-Regular                2\n",
      "Avenir-Heavy                       2\n",
      "MyriadPro-BoldIt                   2\n",
      "Arial-BoldItalicMT                 2\n",
      "Arial-Black                        2\n",
      "PoynterOSDisplay-SemiboldItal      2\n",
      "Agenda-Bold                        2\n",
      "SunSC-SemiBold                     2\n",
      "PoynterOSTextTwoL-Italic           2\n",
      "Sun-Regular                        2\n",
      "Univers-Bold                       2\n",
      "NewBaskervilleStd-Italic           1\n",
      "AkzidenzGroteskBE-LightEx          1\n",
      "AkzidenzGroteskBE-MdEx             1\n",
      "UniversLTStd                       1\n",
      "Univers-CondensedBold              1\n",
      "Univers-Condensed                  1\n",
      "TradeGothicLTStd-Cn18              1\n",
      "Gotham-Bold                        1\n",
      "Gotham-Black                       1\n",
      "NewBaskervilleStd-Bold             1\n",
      "Gotham-Medium                      1\n",
      "Gotham-Thin                        1\n",
      "GothamBold                         1\n",
      "SunSC-Light                        1\n",
      "UniversLTStd-Bold                  1\n",
      "NewBaskervilleStd-Roman            1\n",
      "Veneer                             1\n",
      "Name: date, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "font_days = df_denver_post.groupby(['font_family_weight']).date.nunique().sort_values(ascending=False)\n",
    "\n",
    "print('''Fonts by number of days on which they appear\n",
    "\n",
    "{}\n",
    "'''.format(\n",
    "    font_days\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1229bb2b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEVCAYAAAAfCXWSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHaZJREFUeJzt3XuYHXWd5/H3x06A5mYT7SebdKJhBOMwRBPtARxcB7kY\nLo7JMi4DK0zEaMZdHXHWJxpcZ7wMM4kTV3R3XDUDShwQxBgCi5fIBhhWR1g7Bg0SM0EkkM6thTQg\ntJqE7/5Rvw6nm9PddbrPpU/15/U8/XTVr6pOfX9Vdb6n6lc3RQRmZlYML2p0AGZmVj1O6mZmBeKk\nbmZWIE7qZmYF4qRuZlYgTupmZgXSFEld0nWSrmrQvCXpK5L2Sfp/OcafJSkkTapHfJWqtD5WTJLu\nlvSuIYZ5G2lio0rqkh6RtFfSUSVl75J0d9UiGz/eAJwDzIiIUxodTBVUrT6S3iHp+9UJy0qlHYMT\najX+CLyNNLGx7Km3AFdUK5B6kdRS4SQvBx6JiGdqEU8DFK0+YzZej6oayNvIIE21jURExX/AI8Ay\n4AmgLZW9C7g7dc8CAphUMs3dwLtS9zuAHwBXA73Aw8AfpfLHgL3AopJprwO+CNwBPA38C/DykuGv\nSsOeALYCFw2a9gvAt4FngLPL1Gc6cFua/iHg3al8MfAb4CDwa+ATZaZtAT4N/CrV472ldQcuB7ak\nuB8G/qJk2geAPynpn5w+Zx5wBHA98HhaRj8Cpg6xPpYBv0jzeBD4D0OMV7Y+wLtTvZ9Iy2F6yTQB\nvAfYluL4PCDg9wd9Vu8Q8xyu/mcAO4CPpHo/Ary9Suv9AmAT8FTapj5eMmxWqtdi4FHgnkqW96D6\nfQd436CynwAXpuV0Ndn2/BSwGTg5x2fek+J7Ji3bPxtuPZUbHzgOuB3oAfal7hnlvo/eRvJtI6n8\nG8Bu4Mm03P9gUCyfB76VYrkPeEXJ8D8oiWUP8JFU/iKe/w4/DtwMTEnDKt4ux5LUzwbWAlfF6JL6\ngbQyW4Cr0oL7PHA48Oa0UI4uWVhPA29Mwz8HfD8NOyqtkMuBSWQJ8VfASSXTPgmcnhbeEUN8if5X\nWoBzyb4IZ5bE+v1hlsV7gJ8DM4EpwF0MTOoXAK8g28j/GHgWeG0a9iHg6yWftQDYnLr/AvjfwJFp\nGb0OOHaIGP4j2Q/Ti8i+0M8A04YYd0B9gDPT8nptWrb/k7QBl3xhbwfagJelZXNunmWTo/5npO3g\nM2nef5xin12F9X4GMCctk1eTfYkWDto+v5o+p7WS5T2ofn8O/KCk/ySyL9/hwHxgY1p2/Umu7Hop\n87kBnFDheiod/yXAn6b6HEOWjNaNlNS9jQy9jaTyd6bleTjwWeD+QUn9ceCUNL8bgJvSsGOAXcAH\nyfLMMcCpadgVwL3AjPS5XwJurDQPVCupn0yWMNupPKlvKxk2J40/taTscWBuycK6qWTY0WS//jPJ\nktj/HRTfl4CPlUz71WHqMjN91jElZcuB6/JslMCdwHtK+t88uO6Dxl8HXJG6p5NtkMem/jXAh0o2\nnn8FXj2K9XM/sCDnF/Za4B8GLdv9wKySL+wbSobfDCzL+4Udof5nkH1hjxr0+X891vVeZr6fBa4e\ntH3+XsnwUS1vsi/nM6S9Q+DvgC+n7jOBfwNOA15U4ecOTtJ51tMJw3zeXGBfSf/d5E/q3kbKj9+W\nxnlxSSzXlAw/H/h56r4E2DTE52wBzirpn5aW76TRbJdjuvolIh4g+4VeNorJ95R096XPG1x2dEn/\nYyXz/TXZIcx0sva/UyX19v8Bbwf+Xblpy5gOPBERT5eUbQc6ctZj+qDP3146UNJ5ku6V9ESK7Xzg\npakeO8maof5UUhtwHtmvO8A/A+uBmyTtlPQPkiaXC0DSn0u6v6T+J/fPI2f8h2JOy/ZxBtZ/d0n3\nswxcL8Marv7JvhjYdrs9xdRvVOtd0qmS7pLUI+lJsiOqwcukdL3lXt6l0nbzLeDiVHQJaR1GxJ3A\nP5Idge6VtErSsSN95hDyrKdDJB0p6UuStkt6iuxotG0U55Tyzrvw24ikFkkrJP0iLdNH0qDSaYZa\nDjPJmlfKeTlwS0mMW8h+mKYyiu2yGpc0foysva10BfevgCNLykqT7GjM7O+QdDRZU8dOsoX+LxHR\nVvJ3dET855JpY5jP3QlMkXRMSdnLgO6cce0qjS1N2x/n4cA3ydrcp0ZEG1nbvkrGXw1cStaE8sOI\n6AaIiP0R8YmIOInsfMNbyA71B5D0cuCfgPcBL0nzeGDQPIazk2yj6v+8o8gO3fPUf7jlmrf+x5Ve\nRUW2/HaW9I92vX+NrO13ZkS8mKzddfAyORR/3uU9hBuBSyS9nuzQ+q6Sz/0fEfE6smaZVwJLc37m\nYJWupw8Cs8kO8Y8la56A/NvFWOZdqjDbCPCfyJpIzwZeTLY3T5lpynkM+L1hhp03KM4jIqJ7NNvl\nmJN6RDwEfB14f0lZD9kKvzT9ur2TrM1sLM6X9AZJhwF/C9wbEY+RHSm8UtJlkianvz+U9Ps543+M\n7PBmuaQjJL2a7OTI9Tnjuhl4v6QZko5j4FHLYWRtZD3AAUnnkTXPlFpH1lZ5BVn7HQCS3iRpTtqz\neorscOy5MvM/imzD60nTXU62p57XjcDlkuamL9jfA/dFxCM5pt0DzEjrpJw89Qf4hKTDJP17so32\nGyXDRrvejyE7AvuNpFPIvpBDGm55S/r4CJfrfpss6X2S7BxJ/3R/mPYGJ5Pt6PyG8uuwnD0MTAIj\nrafB4x9DdrTbK2kK2c7XaHkbeX7835IdpRxJthzyuh2YJukDkg6XdIykU9OwLwJ/l3bQkNQuaUHq\nzpsHDqnWzUefJEsupd5NtlfyONlZ338d4zy+RrZhPkF2suBSOHT4+2ayw9+dZIc/nyLbUPK6hOxX\ndydwC1mb2//JOe0/kR0e/QT4MdnJY0piez9Z4t9HttHcVjpxRPSR7akcXzot2ZHNGrIVuYXsrP4/\nD555RDwI/Hfgh2RfoDlkTTq5pHr+dYphF9mP78XDTvS8O4GfAbsl/arMZ49Yf7L1tY9s2d9Adn7i\n5yXDR7ve/wvwSUlPA3+TYhjOcMt7JsMs04j4Ldm6OzvF2+9Ysu1jH1mTwePASgBJH5H0nWHi+Tiw\nOh2SX5RjPQ0Yn6x9uJXsxOC9wHeHr/7QvI0c8lWy9dhNdpXZvSOMP7ie5wB/kuLYBrwpDf5cqvP3\nUiz3Av0JP1ceKKXUMG8NJOlvgFdGxKWNjqWeJJ0BXB8RM4YYfh2wIyI+Ws+4ysRxP9mJrMcbGcdE\n1CzbyHjSPBfUF1Q6NF4MXNboWKy8iJjb6BjM8mqKZ78UlaR3k50k+U5E3NPoeMys+bn5xcysQLyn\nbmZWIE7qZmYF4qRuZlYgTupmZgXipG5mViBO6mZmBeKkbmZWIE7qZmYF4qRuZlYgTupmZgXipG5m\nViBO6mZmBeKkbmZWIE7qZmYFUteXZLz0pS+NWbNm1XOWZmZNb+PGjb+KiPY849Y1qc+aNYuurq56\nztLMrOlJ2p53XDe/mJkViJO6mVmBOKmbmRWIk7qZWYE4qZuZFUiuq18k/RXwLiCAzcDlwDTgJuAl\nwEbgsoj4XY3iNDNrOus2dbNy/VZ29vYxva2VpfNns3BeR03nOeKeuqQO4P1AZ0ScDLQAFwOfAq6O\niBOAfcDiWgZqZtZM1m3q5sq1m+nu7SOA7t4+rly7mXWbums637zNL5OAVkmTgCOBXcCZwJo0fDWw\nsPrhmZk1p5Xrt9K3/+CAsr79B1m5fmtN5ztiUo+IbuDTwKNkyfxJsuaW3og4kEbbAZQ9ppC0RFKX\npK6enp7qRG1mNs7t7O2rqLxa8jS/HAcsAI4HpgNHAefmnUFErIqIzojobG/PdZermVnTm97WWlF5\nteRpfjkb+GVE9ETEfmAtcDrQlppjAGYAtW0oMjNrIkvnz6Z1csuAstbJLSydP7um882T1B8FTpN0\npCQBZwEPAncBb0vjLAJurU2IZmbNZ+G8DpZfOIeOtlYEdLS1svzCOTW/+kURMfJI0ieAPwMOAJvI\nLm/sILukcUoquzQifjvc53R2doYf6GVmVhlJGyOiM8+4ua5Tj4iPAR8bVPwwcEqFsZmZWQ35jlIz\nswJxUjczKxAndTOzAnFSNzMrECd1M7MCcVI3MysQJ3UzswJxUjczKxAndTOzAnFSNzMrECd1M7MC\ncVI3MysQJ3UzswJxUjczKxAndTOzAnFSNzMrkDwvnp4t6f6Sv6ckfUDSFEl3SNqW/h9Xj4DNzGxo\nIyb1iNgaEXMjYi7wOuBZ4BZgGbAhIk4ENqR+MzNroEqbX84CfhER24EFwOpUvhpYWM3AzMyscpUm\n9YuBG1P31IjYlbp3A1PLTSBpiaQuSV09PT2jDNPMzPLIndQlHQa8FfjG4GEREUCUmy4iVkVEZ0R0\ntre3jzpQMzMbWSV76ucBP46IPal/j6RpAOn/3moHZ2ZmlakkqV/C800vALcBi1L3IuDWagVlZmaj\nkyupSzoKOAdYW1K8AjhH0jbg7NRvZmYNNCnPSBHxDPCSQWWPk10NY2Zm44TvKDUzKxAndTOzAnFS\nNzMrECd1M7MCcVI3MysQJ3UzswJxUjczKxAndTOzAnFSNzMrECd1M7MCcVI3MysQJ3UzswJxUjcz\nKxAndTOzAnFSNzMrECd1M7MCyfvmozZJayT9XNIWSa+XNEXSHZK2pf/H1TpYMzMbXt499c8B342I\nVwGvAbYAy4ANEXEisCH1m5lZA42Y1CW9GHgjcC1ARPwuInqBBcDqNNpqYGGtgjQzs3zy7KkfD/QA\nX5G0SdI16UXUUyNiVxpnNzC13MSSlkjqktTV09NTnajNzKysPEl9EvBa4AsRMQ94hkFNLRERQJSb\nOCJWRURnRHS2t7ePNV4zMxtGnqS+A9gREfel/jVkSX6PpGkA6f/e2oRoZmZ5jZjUI2I38Jik2ano\nLOBB4DZgUSpbBNxakwjNzCy3STnH+0vgBkmHAQ8Dl5P9INwsaTGwHbioNiGamVleuZJ6RNwPdJYZ\ndFZ1wzEzs7HwHaVmZgXipG5mViBO6mZmBeKkbmZWIE7qZmYF4qRuZlYgTupmZgXipG5mViBO6mZm\nBeKkbmZWIE7qZmYF4qRuZlYgTupmZgXipG5mViBO6mZmBeKkbmZWILlekiHpEeBp4CBwICI6JU0B\nvg7MAh4BLoqIfbUJ08ys+azb1M3K9VvZ2dvH9LZWls6fzcJ5HTWdZyV76m+KiLkR0f8GpGXAhog4\nEdiQ+s3MjCyhX7l2M929fQTQ3dvHlWs3s25Td03nO5bmlwXA6tS9Glg49nDMzIph5fqt9O0/OKCs\nb/9BVq7fWtP55k3qAXxP0kZJS1LZ1IjYlbp3A1PLTShpiaQuSV09PT1jDNfMrDns7O2rqLxa8ib1\nN0TEa4HzgPdKemPpwIgIssT/AhGxKiI6I6Kzvb19bNGamTWJ6W2tFZVXS66kHhHd6f9e4BbgFGCP\npGkA6f/eWgVpZtZsls6fTevklgFlrZNbWDp/dk3nO2JSl3SUpGP6u4E3Aw8AtwGL0miLgFtrFaSZ\nWbNZOK+D5RfOoaOtFQEdba0sv3BOza9+yXNJ41TgFkn9438tIr4r6UfAzZIWA9uBi2oXpplZ81k4\nr6PmSXywEZN6RDwMvKZM+ePAWbUIyszMRsd3lJqZFYiTuplZgTipm5kViJO6mVmBOKmbmRWIk7qZ\nWYE4qZuZFYiTuplZgTipm5kViJO6mVmBOKmbmRWIk7qZWYE4qZuZFYiTuplZgTipm5kViJO6mVmB\n5E7qklokbZJ0e+o/XtJ9kh6S9HVJh9UuTDMzy6OSPfUrgC0l/Z8Cro6IE4B9wOJqBmZmZpXLldQl\nzQAuAK5J/QLOBNakUVYDC2sRoJmZ5Zd3T/2zwIeA51L/S4DeiDiQ+ncAZd+uKmmJpC5JXT09PWMK\n1szMhjdiUpf0FmBvRGwczQwiYlVEdEZEZ3t7+2g+wszMcpqUY5zTgbdKOh84AjgW+BzQJmlS2luf\nAXTXLkwzM8tjxD31iLgyImZExCzgYuDOiHg7cBfwtjTaIuDWmkVpZma5jOU69Q8D/1XSQ2Rt7NdW\nJyQzMxutPM0vh0TE3cDdqfth4JTqh2RmZqPlO0rNzArESd3MrECc1M3MCsRJ3cysQJzUzcwKxEnd\nzKxAKrqk0czM8lu3qZuV67eys7eP6W2tLJ0/m4Xzyj4mq2qc1M3MamDdpm6uXLuZvv0HAeju7ePK\ntZsBaprY3fxiZlYDK9dvPZTQ+/XtP8jK9VtrOl8ndTOzGtjZ21dRebU4qZuZ1UDbkZMrKq8WJ3Uz\nsxqIqKy8WpzUzcxq4Mm+/RWVV4uTuplZDUxva62ovFqc1M3MamDp/Nm0Tm4ZUNY6uYWl82fXdL6+\nTt3MrAb6r0UfdzcfSToCuAc4PI2/JiI+Jul44Caytx5tBC6LiN/VMlgzs2aycF5HzZP4YHmaX34L\nnBkRrwHmAudKOg34FHB1RJwA7AMW1y5MMzPLI8+LpyMifp16J6e/AM4E1qTy1cDCmkRoZma55TpR\nKqlF0v3AXuAO4BdAb0QcSKPsAMoeY0haIqlLUldPT081YjYzsyHkSuoRcTAi5gIzyF42/aq8M4iI\nVRHRGRGd7e3towzTzMzyqOiSxojoBe4CXg+0Seo/0ToD6K5ybGZmVqERk7qkdkltqbsVOAfYQpbc\n35ZGWwTcWqsgzcwsnzzXqU8DVktqIfsRuDkibpf0IHCTpKuATcC1NYzTzMxyGDGpR8RPgXllyh8m\na183M7Nxwo8JMDMrED8mwMwmlEa8N7SenNTNbMJo1HtD68nNL2Y2YTTqvaH15KRuZhNGo94bWk9O\n6mY2YTTqxRX15KRuZhNGo15cUU8+UWpmE0ajXlxRT07qZjahNOLFFfXk5hczswJxUjczKxA3vzRY\n0e9uM7P6clJvoIlwd5uZ1ZebXxpoItzdZmb15aTeQBPh7jYzqy8n9QaaCHe3mVl95Xmd3UxJd0l6\nUNLPJF2RyqdIukPStvT/uNqHWywT4e42M6uvPHvqB4APRsRJwGnAeyWdBCwDNkTEicCG1G8VWDiv\ng+UXzqGjrRUBHW2tLL9wjk+Smtmo5Xmd3S5gV+p+WtIWoANYAJyRRlsN3A18uCZRFljR724zs/qq\n6JJGSbPI3ld6HzA1JXyA3cDUIaZZAiwBeNnLXjbaOM2soD66bjM33vcYByNokbjk1JlctXBOo8Nq\nWrlPlEo6Gvgm8IGIeKp0WEQEEOWmi4hVEdEZEZ3t7e1jCtbMiuWj6zZz/b2PcjCy9HEwguvvfZSP\nrtvc4MiaV66kLmkyWUK/ISLWpuI9kqal4dOAvbUJ0cyK6sb7Hquo3EaW5+oXAdcCWyLiMyWDbgMW\npe5FwK3VD8/Miqx/Dz1vuY0sT5v66cBlwGZJ96eyjwArgJslLQa2AxfVJkQzK6oWqWwCb5EaEE0x\n5Ln65fvAUEv4rOqGY2YTySWnzuT6ex8tW26j4wd6mVnD9F/l4qtfqkdRx7arzs7O6Orqqtv8bCA/\n5tesvqr1nZO0MSI684zrPfUJwo/5NauvRn3n/ECvCcKP+TWrr0Z955zUJwg/5tesvhr1nXNSnyD8\nmF+z+mo7cnJF5dXipD5B+DG/Nl6t29TN6Svu5Phl3+L0FXeyblN3o0OqiqGuQan1tSk+UTpB9J+Y\n8dUvNp4U+QT+k337KyqvFif1CcSP+bXxZriTic2+rU5va6W7TPt5rZs83fxiZgPUszmkyCfwG9Xk\n6T11Mzuk3s0hjdqbrYdGNXl6T93MDqn3tdVL589mcsvAR0tNbpFP4I+B99TN7JCGNIcMvhqkIE/d\n9R2lZtZw9b6fYeX6rex/bmAW3/9cFOJOZ99RamYNV+/mkCKfKPUdpWY2PtSxOaTIdzo3qm55Xmf3\nZUl7JT1QUjZF0h2StqX/x9U0SrNxpqh3Qda7OaTIdzo3qm559tSvA84dVLYM2BARJwIbUr/ZhNB/\nAqy7t4/g+RNgRUjs9W4yWDivg+UXzqGjrRUBHW2tLL9wTtPfeASNq1ue19ndI2nWoOIFwBmpezVw\nN/DhKsZlNm75LsjqKvKdzo2o22jb1KdGxK7UvRuYOtSIkpZI6pLU1dPTM8rZmY0fRT65V+TmkIli\nzCdKI3sf3pCnUiJiVUR0RkRne3v7WGdn1nBFPrlX5OaQiWK0Nx/tkTQtInZJmgbsrWZQZuPZ0vmz\nWbrmJ+w/+Py+TJHugixyc8hEMNo99duARal7EXBrdcIxaxIFvQvSml+eSxpvBH4IzJa0Q9JiYAVw\njqRtwNmp32xCKPJdkNb88lz9cskQg86qcixmTaHIJ0qt+fmOUrMKFflEqTW/cf+UxnWbuuv6POJ6\nz6/Iirosl86fPeDpe+DL/mz8GNdJvd6Prizy+xLrrcjL0u97tfFMUetXW5fo7OyMrq6u3OOfvuLO\nsne3dbS18oNlZ1YztIbMr8i8LM2qR9LGiOjMM+64blOv9wkpnwCrHi9Ls8YY10m93iekfAKserws\nzRpjXCf1ej+wf+n82S9YIC9K5bVSz0e41nNefoaIWWOM66QO1PXOva7tT/DcoLLnUnkt1PMRrvV+\nXKyfIWLWGD5RWuIVV36bg2WWR4vEL5afX/X51bN+PnFp1rx8onSUyiX04crHqp7184lLs4lhXCf1\nSUNEN1T5WEmVlY9VPU8m+sSl2cQwrpP6/sEN3COUj1XrEL8WQ5WPVT1PJvrEpdnEMK7vKK23Z4f4\ntRiqfKzqeWei74I0mxic1Busni8k8MsPzIpvXDe/mJlZZZzUzcwKZExJXdK5krZKekjSsmoFZWZm\nozPqpC6pBfg8cB5wEnCJpJOqFVgjPLLigorKzczGm7GcKD0FeCgiHgaQdBOwAHiwGoE1ihO4mTWz\nsTS/dACPlfTvSGUDSFoiqUtSV09PT0UzGOqenxrdC2Rm1vRqfqI0IlZFRGdEdLa3t1c07S9XXPCC\nBK5UbmZmLzSW5pduYGZJ/4xUVlVO4GZm+Y1lT/1HwImSjpd0GHAxcFt1wjIzs9EY9Z56RByQ9D5g\nPdACfDkifla1yMzMrGJjekxARHwb+HaVYjEzszHyHaVmZgXipG5mViB1fZ2dpB5ge91mODYvBX7V\n6CBqxHVrXkWun+s2tJdHRK5rwuua1JuJpK687wRsNq5b8ypy/Vy36nDzi5lZgTipm5kViJP60FY1\nOoAact2aV5Hr57pVgdvUzcwKxHvqZmYF4qReQtJMSXdJelDSzyRd0eiYqk1Si6RNkm5vdCzVJqlN\n0hpJP5e0RdLrGx1TtUj6q7RNPiDpRklHNDqmsZD0ZUl7JT1QUjZF0h2StqX/xzUyxtEaom4r03b5\nU0m3SGqr1fyd1Ac6AHwwIk4CTgPe2+xvcyrjCmBLo4Ookc8B342IVwGvoSD1lNQBvB/ojIiTyZ61\ndHFjoxqz64BzB5UtAzZExInAhtTfjK7jhXW7Azg5Il4N/BtwZa1m7qReIiJ2RcSPU/fTZEnhBS/+\naFaSZgAXANc0OpZqk/Ri4I3AtQAR8buI6G1sVFU1CWiVNAk4EtjZ4HjGJCLuAZ4YVLwAWJ26VwML\n6xpUlZSrW0R8LyIOpN57yR5VXhNO6kOQNAuYB9zX2Eiq6rPAh4DnGh1IDRwP9ABfSc1L10g6qtFB\nVUNEdAOfBh4FdgFPRsT3GhtVTUyNiF2pezcwtZHB1NA7ge/U6sOd1MuQdDTwTeADEfFUo+OpBklv\nAfZGxMZGx1Ijk4DXAl+IiHnAMzTv4fsAqW15AdkP13TgKEmXNjaq2orssrzCXZon6b+RNfPeUKt5\nOKkPImkyWUK/ISLWNjqeKjodeKukR4CbgDMlXd/YkKpqB7AjIvqPrNaQJfkiOBv4ZUT0RMR+YC3w\nRw2OqRb2SJoGkP7vbXA8VSXpHcBbgLdHDa8ld1IvIUlkbbJbIuIzjY6nmiLiyoiYERGzyE6y3RkR\nhdnbi4jdwGOSZqeis4AHGxhSNT0KnCbpyLSNnkVBTgIPchuwKHUvAm5tYCxVJelcsqbPt0bEs7Wc\nl5P6QKcDl5Htxd6f/s5vdFCW218CN0j6KTAX+PsGx1MV6ehjDfBjYDPZ97ap776UdCPwQ2C2pB2S\nFgMrgHMkbSM7OlnRyBhHa4i6/SNwDHBHyitfrNn8fUepmVlxeE/dzKxAnNTNzArESd3MrECc1M3M\nCsRJ3cysQJzUzcwKxEndzKxAnNTNzArk/wNvXQKpOK2jXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1215c8f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font_stats['days_present'] = font_days\n",
    "\n",
    "plt.suptitle('Number of days a font appears, vs. total font appearances')\n",
    "plt.scatter(font_stats.days_present, font_stats['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fontface</th>\n",
       "      <th>fontsize</th>\n",
       "      <th>bbox_left</th>\n",
       "      <th>bbox_bottom</th>\n",
       "      <th>bbox_right</th>\n",
       "      <th>bbox_top</th>\n",
       "      <th>bbox_area</th>\n",
       "      <th>avg_character_area</th>\n",
       "      <th>percent_of_page</th>\n",
       "      <th>...</th>\n",
       "      <th>page_area</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>slug</th>\n",
       "      <th>id</th>\n",
       "      <th>font_family_weight</th>\n",
       "      <th>font_leading_thing</th>\n",
       "      <th>font_family</th>\n",
       "      <th>font_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79644</th>\n",
       "      <td>Running hot and coal</td>\n",
       "      <td>HGLMOI+PoynterOSDisplay-Roman</td>\n",
       "      <td>71.424</td>\n",
       "      <td>191.60</td>\n",
       "      <td>1121.558</td>\n",
       "      <td>712.886</td>\n",
       "      <td>1192.982</td>\n",
       "      <td>37232.331264</td>\n",
       "      <td>2241.201092</td>\n",
       "      <td>0.033815</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>269992</td>\n",
       "      <td>PoynterOSDisplay-Roman</td>\n",
       "      <td>HGLMOI</td>\n",
       "      <td>PoynterOSDisplay</td>\n",
       "      <td>Roman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79632</th>\n",
       "      <td>N. Korea\\nmissile\\nlaunch\\na failure</td>\n",
       "      <td>HGLLEM+PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>40.752</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1027.350</td>\n",
       "      <td>142.956</td>\n",
       "      <td>1176.102</td>\n",
       "      <td>18587.454912</td>\n",
       "      <td>637.966738</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>269980</td>\n",
       "      <td>PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>HGLLEM</td>\n",
       "      <td>PoynterOSDisplayNarrow</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79627</th>\n",
       "      <td>DEBATING THE FUTURE\\nOF LEGAL MARIJUANA » 1-6D</td>\n",
       "      <td>HGLKPL+Sun-SemiBold</td>\n",
       "      <td>34.432</td>\n",
       "      <td>382.76</td>\n",
       "      <td>1374.210</td>\n",
       "      <td>679.864</td>\n",
       "      <td>1438.642</td>\n",
       "      <td>19143.004928</td>\n",
       "      <td>509.717903</td>\n",
       "      <td>0.017386</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>269975</td>\n",
       "      <td>Sun-SemiBold</td>\n",
       "      <td>HGLKPL</td>\n",
       "      <td>Sun</td>\n",
       "      <td>SemiBold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79647</th>\n",
       "      <td>FLORAL TRADITION</td>\n",
       "      <td>HGLMOK+PoynterOSDisplay-Semibold</td>\n",
       "      <td>27.048</td>\n",
       "      <td>191.60</td>\n",
       "      <td>609.376</td>\n",
       "      <td>425.622</td>\n",
       "      <td>636.424</td>\n",
       "      <td>6329.827056</td>\n",
       "      <td>417.750950</td>\n",
       "      <td>0.005749</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>269995</td>\n",
       "      <td>PoynterOSDisplay-Semibold</td>\n",
       "      <td>HGLMOK</td>\n",
       "      <td>PoynterOSDisplay</td>\n",
       "      <td>Semibold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79623</th>\n",
       "      <td>M EM B E R S</td>\n",
       "      <td>HGLLBF+TheSerifLight-Plain</td>\n",
       "      <td>22.232</td>\n",
       "      <td>35.40</td>\n",
       "      <td>1440.360</td>\n",
       "      <td>104.252</td>\n",
       "      <td>1462.592</td>\n",
       "      <td>1530.717664</td>\n",
       "      <td>202.666912</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-16</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>269971</td>\n",
       "      <td>TheSerifLight-Plain</td>\n",
       "      <td>HGLLBF</td>\n",
       "      <td>TheSerifLight</td>\n",
       "      <td>Plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22731</th>\n",
       "      <td>A different path</td>\n",
       "      <td>FDDFNB+PoynterOSDisplay-Roman</td>\n",
       "      <td>89.280</td>\n",
       "      <td>36.48</td>\n",
       "      <td>1157.300</td>\n",
       "      <td>534.640</td>\n",
       "      <td>1246.580</td>\n",
       "      <td>44475.724800</td>\n",
       "      <td>3198.264686</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>213078</td>\n",
       "      <td>PoynterOSDisplay-Roman</td>\n",
       "      <td>FDDFNB</td>\n",
       "      <td>PoynterOSDisplay</td>\n",
       "      <td>Roman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22681</th>\n",
       "      <td>Denver\\nto pay\\n$1M for\\ndeath</td>\n",
       "      <td>FDDFLK+PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>50.374</td>\n",
       "      <td>597.20</td>\n",
       "      <td>1058.160</td>\n",
       "      <td>729.815</td>\n",
       "      <td>1242.034</td>\n",
       "      <td>24384.450510</td>\n",
       "      <td>1077.820422</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>213081</td>\n",
       "      <td>PoynterOSDisplayNarrow-Bold</td>\n",
       "      <td>FDDFLK</td>\n",
       "      <td>PoynterOSDisplayNarrow</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22756</th>\n",
       "      <td>303-900-2926</td>\n",
       "      <td>FDDHLE+Univers-Bold</td>\n",
       "      <td>38.265</td>\n",
       "      <td>498.78</td>\n",
       "      <td>87.241</td>\n",
       "      <td>699.938</td>\n",
       "      <td>125.507</td>\n",
       "      <td>7697.512028</td>\n",
       "      <td>639.485448</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>213106</td>\n",
       "      <td>Univers-Bold</td>\n",
       "      <td>FDDHLE</td>\n",
       "      <td>Univers</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22725</th>\n",
       "      <td>ROCKIES GET ONLY THREE HITS IN 6-0 LOSS TO PAD...</td>\n",
       "      <td>FDDFIM+Sun-SemiBold</td>\n",
       "      <td>34.432</td>\n",
       "      <td>30.22</td>\n",
       "      <td>1429.810</td>\n",
       "      <td>717.780</td>\n",
       "      <td>1464.242</td>\n",
       "      <td>23674.065920</td>\n",
       "      <td>519.158062</td>\n",
       "      <td>0.021501</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>213072</td>\n",
       "      <td>Sun-SemiBold</td>\n",
       "      <td>FDDFIM</td>\n",
       "      <td>Sun</td>\n",
       "      <td>SemiBold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22751</th>\n",
       "      <td>Learn about\\nreplacement\\nwindows before\\nyou ...</td>\n",
       "      <td>FDDHLB+Agenda-Bold</td>\n",
       "      <td>31.870</td>\n",
       "      <td>32.46</td>\n",
       "      <td>37.699</td>\n",
       "      <td>204.481</td>\n",
       "      <td>157.600</td>\n",
       "      <td>20625.489921</td>\n",
       "      <td>430.110672</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>...</td>\n",
       "      <td>1101056.0</td>\n",
       "      <td>2017-04-13</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>CO_DP</td>\n",
       "      <td>213101</td>\n",
       "      <td>Agenda-Bold</td>\n",
       "      <td>FDDHLB</td>\n",
       "      <td>Agenda</td>\n",
       "      <td>Bold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "79644                               Running hot and coal   \n",
       "79632               N. Korea\\nmissile\\nlaunch\\na failure   \n",
       "79627     DEBATING THE FUTURE\\nOF LEGAL MARIJUANA » 1-6D   \n",
       "79647                                   FLORAL TRADITION   \n",
       "79623                                       M EM B E R S   \n",
       "22731                                   A different path   \n",
       "22681                     Denver\\nto pay\\n$1M for\\ndeath   \n",
       "22756                                       303-900-2926   \n",
       "22725  ROCKIES GET ONLY THREE HITS IN 6-0 LOSS TO PAD...   \n",
       "22751  Learn about\\nreplacement\\nwindows before\\nyou ...   \n",
       "\n",
       "                                 fontface  fontsize  bbox_left  bbox_bottom  \\\n",
       "79644       HGLMOI+PoynterOSDisplay-Roman    71.424     191.60     1121.558   \n",
       "79632  HGLLEM+PoynterOSDisplayNarrow-Bold    40.752      18.00     1027.350   \n",
       "79627                 HGLKPL+Sun-SemiBold    34.432     382.76     1374.210   \n",
       "79647    HGLMOK+PoynterOSDisplay-Semibold    27.048     191.60      609.376   \n",
       "79623          HGLLBF+TheSerifLight-Plain    22.232      35.40     1440.360   \n",
       "22731       FDDFNB+PoynterOSDisplay-Roman    89.280      36.48     1157.300   \n",
       "22681  FDDFLK+PoynterOSDisplayNarrow-Bold    50.374     597.20     1058.160   \n",
       "22756                 FDDHLE+Univers-Bold    38.265     498.78       87.241   \n",
       "22725                 FDDFIM+Sun-SemiBold    34.432      30.22     1429.810   \n",
       "22751                  FDDHLB+Agenda-Bold    31.870      32.46       37.699   \n",
       "\n",
       "       bbox_right  bbox_top     bbox_area  avg_character_area  \\\n",
       "79644     712.886  1192.982  37232.331264         2241.201092   \n",
       "79632     142.956  1176.102  18587.454912          637.966738   \n",
       "79627     679.864  1438.642  19143.004928          509.717903   \n",
       "79647     425.622   636.424   6329.827056          417.750950   \n",
       "79623     104.252  1462.592   1530.717664          202.666912   \n",
       "22731     534.640  1246.580  44475.724800         3198.264686   \n",
       "22681     729.815  1242.034  24384.450510         1077.820422   \n",
       "22756     699.938   125.507   7697.512028          639.485448   \n",
       "22725     717.780  1464.242  23674.065920          519.158062   \n",
       "22751     204.481   157.600  20625.489921          430.110672   \n",
       "\n",
       "       percent_of_page     ...      page_area       date  day_of_week  \\\n",
       "79644         0.033815     ...      1101056.0 2017-04-16            6   \n",
       "79632         0.016881     ...      1101056.0 2017-04-16            6   \n",
       "79627         0.017386     ...      1101056.0 2017-04-16            6   \n",
       "79647         0.005749     ...      1101056.0 2017-04-16            6   \n",
       "79623         0.001390     ...      1101056.0 2017-04-16            6   \n",
       "22731         0.040394     ...      1101056.0 2017-04-13            3   \n",
       "22681         0.022146     ...      1101056.0 2017-04-13            3   \n",
       "22756         0.006991     ...      1101056.0 2017-04-13            3   \n",
       "22725         0.021501     ...      1101056.0 2017-04-13            3   \n",
       "22751         0.018732     ...      1101056.0 2017-04-13            3   \n",
       "\n",
       "       weekend   slug      id           font_family_weight font_leading_thing  \\\n",
       "79644     True  CO_DP  269992       PoynterOSDisplay-Roman             HGLMOI   \n",
       "79632     True  CO_DP  269980  PoynterOSDisplayNarrow-Bold             HGLLEM   \n",
       "79627     True  CO_DP  269975                 Sun-SemiBold             HGLKPL   \n",
       "79647     True  CO_DP  269995    PoynterOSDisplay-Semibold             HGLMOK   \n",
       "79623     True  CO_DP  269971          TheSerifLight-Plain             HGLLBF   \n",
       "22731    False  CO_DP  213078       PoynterOSDisplay-Roman             FDDFNB   \n",
       "22681    False  CO_DP  213081  PoynterOSDisplayNarrow-Bold             FDDFLK   \n",
       "22756    False  CO_DP  213106                 Univers-Bold             FDDHLE   \n",
       "22725    False  CO_DP  213072                 Sun-SemiBold             FDDFIM   \n",
       "22751    False  CO_DP  213101                  Agenda-Bold             FDDHLB   \n",
       "\n",
       "                  font_family font_weight  \n",
       "79644        PoynterOSDisplay       Roman  \n",
       "79632  PoynterOSDisplayNarrow        Bold  \n",
       "79627                     Sun    SemiBold  \n",
       "79647        PoynterOSDisplay    Semibold  \n",
       "79623           TheSerifLight       Plain  \n",
       "22731        PoynterOSDisplay       Roman  \n",
       "22681  PoynterOSDisplayNarrow        Bold  \n",
       "22756                 Univers        Bold  \n",
       "22725                     Sun    SemiBold  \n",
       "22751                  Agenda        Bold  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_denver_post.sort_values(['date', 'avg_character_area'], ascending=False).groupby('date').head(5).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram \"percent of page\" analysis\n",
    "\n",
    "Given an unigram like \"Syria\", how much of a given front page does it occupy? \n",
    "\n",
    "### Notes\n",
    "\n",
    "We will consider the entire text block that contains the unigram to be related to that unigram. For example, the entire headline of \"US BOMBS SYRIA\" will be counted as space devoted toward \"Syria\". Likewise, a lengthy front-page article that mentions \"Syria\" in it will (naively, perhaps) be considered 100% about Syria.\n",
    "\n",
    "We're assuming that search queries will be proper nouns, so we're not going to perform any stemming or lemmatizing.\n",
    "\n",
    "### Followup approaches\n",
    "\n",
    "Some newspapers contain more and smaller text, like the NYT, compared to tabloids where words are written extremely largely across the surface. This may still be of interest -- we do want to acknowledge the space devoted to \"Syria\" if it is splashed across the front of the tabloid -- but we may also want to develop a measure of relative importance so that a top-of-banner headline is weighted equally across all newspapers.\n",
    "\n",
    "This approach does not touch on probabilistic topic modeling yet -- these are only direct matches.\n",
    "\n",
    "We will also want to develop a method to link a headline with an article, so that a headline like \"BOOTS ON THE GROUND\" could possibly be linked to the followup article on Syria. This would also allow us to do some tangential but interesting accounts of which Associated Press articles get republished the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "import pprint\n",
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "chars = set(string.ascii_letters)\n",
    "\n",
    "def include_word(word):\n",
    "    return sum([c in chars for c in word]) >= 3\n",
    "\n",
    "def preprocess_text(text):\n",
    "    lowered = text.strip().lower()\n",
    "    lowered = ''.join(lowered.split('-\\n'))\n",
    "    lowered = lowered.replace('\\n', ' ')\n",
    "    words = word_tokenize(lowered)\n",
    "    filtered_words = [word for word in words if include_word(word)]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "def bag_of_words(text):\n",
    "    '''Literally, this returns a set of the bag of words for fast single-token searches'''\n",
    "    return set(preprocess_text(text))\n",
    "\n",
    "def preprocess_all(texts):\n",
    "    for text in texts:\n",
    "        yield text, preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text preprocessing, we consider a few cases:\n",
      "\n",
      "* Newlines should be stripped\n",
      "* Everything should be lower-cased\n",
      "* We should return a tokenized list\n",
      "* Tokens without a certain number of ascii characters (US-English analysis for now) will be rejected\n",
      "\n",
      "The extraction from PDFs still contains word-continuations across line breaks.\n",
      "For now, we'll consider all lines that end with \"-\" as continuations, and\n",
      "link the text from before and after.\n",
      "\n",
      "Newlines without continuations will be replaced with spaces.\n",
      "\n",
      "Examples:\n",
      "[('Hel-\\nlo, bye\\nnow\\n', ['hello', 'bye', 'now']),\n",
      " ('514,499', []),\n",
      " ('Fresh strike\\non town hit\\nby deadly\\nchemicals',\n",
      "  ['fresh', 'strike', 'town', 'hit', 'deadly', 'chemicals']),\n",
      " ('39', [])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''For text preprocessing, we consider a few cases:\n",
    "\n",
    "* Newlines should be stripped\n",
    "* Everything should be lower-cased\n",
    "* We should return a tokenized list\n",
    "* Tokens without a certain number of ascii characters (US-English analysis for now) will be rejected\n",
    "\n",
    "The extraction from PDFs still contains word-continuations across line breaks.\n",
    "For now, we'll consider all lines that end with \"-\" as continuations, and\n",
    "link the text from before and after.\n",
    "\n",
    "Newlines without continuations will be replaced with spaces.\n",
    "\n",
    "Examples:\n",
    "{}\n",
    "'''.format(\n",
    "    pprint.pformat(list(preprocess_all([\n",
    "        'Hel-\\nlo, bye\\nnow\\n',\n",
    "         *df_denver_post.text.sample(3)\n",
    "    ])))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_us['bow'] = df_us.text.apply(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we write a method to get the percent of page that a unigram occupies, for a particular front page.\n",
      "\n",
      "Syria, Denver Post, latest day: 0\n",
      "garbage input, should be 0: 0\n"
     ]
    }
   ],
   "source": [
    "df_denver_post_latest = df_us[(df_us.slug == 'CO_DP') & (df_us.date == df_us.date.max())]\n",
    "\n",
    "def percent_of_page(unigram, one_paper_df):\n",
    "    unigram = unigram.lower().strip()\n",
    "    lines_with_unigram = one_paper_df[one_paper_df.bow.apply(lambda bag: unigram in bag)]\n",
    "    \n",
    "    return lines_with_unigram.percent_of_page.sum()\n",
    "\n",
    "print('''Now we write a method to get the percent of page that a unigram occupies, for a particular front page.\n",
    "\n",
    "Syria, Denver Post, latest day: {}\n",
    "garbage input, should be 0: {}'''.format(\n",
    "    percent_of_page('Syria', df_denver_post_latest),\n",
    "    percent_of_page('asdflkjasdflasdfkjasdf', df_denver_post_latest)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run this method across all the newspapers, across all days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of newspapers with >3 days: 341\n",
      "\n",
      "(Number of total newspapers: 417)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter down to newspapers with entries with more than 3 days\n",
    "days_of_newspapers = df_us.groupby('slug').date.nunique()\n",
    "\n",
    "df_us_3plus = df_us[df_us.slug.isin(set(days_of_newspapers[days_of_newspapers > 3].index))]\n",
    "\n",
    "print('''Number of newspapers with >3 days: {}\n",
    "\n",
    "(Number of total newspapers: {})\n",
    "'''.format(\n",
    "    df_us_3plus.slug.nunique(),\n",
    "    df_us.slug.nunique()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unigram_percent_of_page(query, dataframe):\n",
    "    return dataframe.groupby(['slug', 'date']).apply(partial(percent_of_page, query))\n",
    "\n",
    "def _reshape_percent_of_day_series(percent_of_page):\n",
    "    return percent_of_page.reset_index().rename(columns={0: 'percent_of_page'})\n",
    "\n",
    "def percent_of_page_by_day(percent_of_page_df):\n",
    "    return _reshape_percent_of_day_series(percent_of_page_df).groupby('date').percent_of_page.mean()\n",
    "\n",
    "def percent_of_papers_with_mention(percent_of_page_df, threshold=0):\n",
    "    percents_by_paper_date = _reshape_percent_of_day_series(percent_of_page_df)\n",
    "    greater_than_thresh = (percents_by_paper_date.groupby(['slug', 'date']).percent_of_page.max() > threshold).reset_index()\n",
    "    \n",
    "    return greater_than_thresh.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average mentions per day\n",
    "syria_results = unigram_percent_of_page('Syria', df_us_3plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of papers that mentioned Syria by day:\n",
      "            percent_of_page\n",
      "date                       \n",
      "2017-04-01         0.018727\n",
      "2017-04-04         0.016667\n",
      "2017-04-05         0.221854\n",
      "2017-04-06         0.287313\n",
      "2017-04-07         0.583039\n",
      "2017-04-08         0.594142\n",
      "2017-04-09         0.282609\n",
      "2017-04-10         0.207469\n",
      "2017-04-11         0.151515\n",
      "2017-04-12         0.163194\n",
      "2017-04-13         0.179389\n",
      "2017-04-14         0.100719\n",
      "2017-04-15         0.048000\n",
      "2017-04-16         0.057778\n",
      "\n",
      "Average percent of newspaper front page devoted to Syria by day:\n",
      "date\n",
      "2017-04-01    0.000591\n",
      "2017-04-04    0.000518\n",
      "2017-04-05    0.005230\n",
      "2017-04-06    0.008735\n",
      "2017-04-07    0.027946\n",
      "2017-04-08    0.031311\n",
      "2017-04-09    0.010841\n",
      "2017-04-10    0.009895\n",
      "2017-04-11    0.005084\n",
      "2017-04-12    0.005361\n",
      "2017-04-13    0.007252\n",
      "2017-04-14    0.002086\n",
      "2017-04-15    0.001283\n",
      "2017-04-16    0.001010\n",
      "Name: percent_of_page, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('''Percent of papers that mentioned Syria by day:\n",
    "{}\n",
    "\n",
    "Average percent of newspaper front page devoted to Syria by day:\n",
    "{}'''.format(\n",
    "    percent_of_papers_with_mention(syria_results),\n",
    "    percent_of_page_by_day(syria_results),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting newspapers with population metadata\n",
    "\n",
    "Short of getting data on readership, we'll try to pull population metadata for the hometown of each newspaper.\n",
    "\n",
    "Edit: See bottom for conclusion. Tldr: it's not great, because there are multiple papers per city, many of which are lesser read. Doh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_population = pd.read_csv('~/data/sub-est2015_all.csv', encoding='ISO-8859-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUMLEV</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>PLACE</th>\n",
       "      <th>COUSUB</th>\n",
       "      <th>CONCIT</th>\n",
       "      <th>PRIMGEO_FLAG</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>NAME</th>\n",
       "      <th>STNAME</th>\n",
       "      <th>CENSUS2010POP</th>\n",
       "      <th>ESTIMATESBASE2010</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>city</th>\n",
       "      <th>place_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35950</th>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>39878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Mankato city</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mankato</td>\n",
       "      <td>mankato, minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40886</th>\n",
       "      <td>157</td>\n",
       "      <td>29</td>\n",
       "      <td>137</td>\n",
       "      <td>28000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>Goss town</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>goss</td>\n",
       "      <td>goss, missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81136</th>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>53000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Milwaukee city</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>milwaukee</td>\n",
       "      <td>milwaukee, wisconsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44352</th>\n",
       "      <td>61</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>42820</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>Livermore town</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>livermore</td>\n",
       "      <td>livermore, new hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23257</th>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>7975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Bonner Springs city</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bonner springs</td>\n",
       "      <td>bonner springs, kansas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SUMLEV  STATE  COUNTY  PLACE  COUSUB  CONCIT  PRIMGEO_FLAG FUNCSTAT  \\\n",
       "35950      61     27     103      0   39878       0             0        F   \n",
       "40886     157     29     137  28000       0       0             1        A   \n",
       "81136      61     55     133      0   53000       0             0        F   \n",
       "44352      61     33       9      0   42820       0             1        I   \n",
       "23257      61     20      91      0    7975       0             0        F   \n",
       "\n",
       "                      NAME         STNAME CENSUS2010POP  ESTIMATESBASE2010  \\\n",
       "35950         Mankato city      Minnesota             0                  0   \n",
       "40886            Goss town       Missouri             0                  0   \n",
       "81136       Milwaukee city      Wisconsin             0                  0   \n",
       "44352       Livermore town  New Hampshire             0                  0   \n",
       "23257  Bonner Springs city         Kansas             0                  0   \n",
       "\n",
       "       POPESTIMATE2010  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  \\\n",
       "35950                0                0                0                0   \n",
       "40886                0                0                0                0   \n",
       "81136                0                0                0                0   \n",
       "44352                0                0                0                0   \n",
       "23257                0                0                0                0   \n",
       "\n",
       "       POPESTIMATE2014  POPESTIMATE2015            city  \\\n",
       "35950                0                0         mankato   \n",
       "40886                0                0            goss   \n",
       "81136                0                0       milwaukee   \n",
       "44352                0                0       livermore   \n",
       "23257                0                0  bonner springs   \n",
       "\n",
       "                     place_name  \n",
       "35950        mankato, minnesota  \n",
       "40886            goss, missouri  \n",
       "81136      milwaukee, wisconsin  \n",
       "44352  livermore, new hampshire  \n",
       "23257    bonner springs, kansas  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cities = df_population[df_population.NAME.str.endswith('city') | df_population.NAME.str.endswith('town')]\n",
    "df_cities['city'] = df_cities.NAME.str.slice(0, -5).str.lower()\n",
    "df_cities['place_name'] = df_cities.city + ', ' + df_cities.STNAME.str.lower()\n",
    "\n",
    "df_cities = df_cities.sort_values('POPESTIMATE2015').groupby('place_name').head(1)\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_abbreviation_to_name = {}\n",
    "with open('files/states.csv') as f:\n",
    "    next(f) # skip header\n",
    "    for line in f:\n",
    "        state, abbrev = line.strip().split(',')\n",
    "        state_abbreviation_to_name[abbrev.strip('\"')] = state.strip('\"').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "us_newspapers_df['place_name'] = us_newspapers_df.city.str.lower() + ', ' + us_newspapers_df.state.apply(state_abbreviation_to_name.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_newspapers_with_pop = pd.merge(us_newspapers_df, df_cities[['place_name', 'POPESTIMATE2015']], how='left', on='place_name', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657 out of 731 newspapers had places found in the census.\n",
      "\n",
      "Examples of ones that didn't:\n",
      "13         anchorage, alaska\n",
      "15            juneau, alaska\n",
      "58       ventura, california\n",
      "105    the villages, florida\n",
      "116           macon, georgia\n",
      "Name: place_name, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''{} out of {} newspapers had places found in the census.\n",
    "\n",
    "Examples of ones that didn't:\n",
    "{}\n",
    "'''.format(\n",
    "    us_newspapers_with_pop.POPESTIMATE2015.count(),\n",
    "    us_newspapers_with_pop.shape[0],\n",
    "    us_newspapers_with_pop[us_newspapers_with_pop.POPESTIMATE2015.isnull()].place_name.head()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>slug</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>website</th>\n",
       "      <th>place_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anniston</td>\n",
       "      <td>USA</td>\n",
       "      <td>33.696739</td>\n",
       "      <td>-85.823433</td>\n",
       "      <td>AL_AS</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Anniston Star</td>\n",
       "      <td>http://www.annistonstar.com</td>\n",
       "      <td>anniston, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>USA</td>\n",
       "      <td>33.518509</td>\n",
       "      <td>-86.804756</td>\n",
       "      <td>AL_BN</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Birmingham News</td>\n",
       "      <td>http://www.al.com/birmingham/</td>\n",
       "      <td>birmingham, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cullman</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.176857</td>\n",
       "      <td>-86.838188</td>\n",
       "      <td>AL_CT</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Cullman Times</td>\n",
       "      <td>http://www.cullmantimes.com</td>\n",
       "      <td>cullman, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decatur</td>\n",
       "      <td>USA</td>\n",
       "      <td>34.602890</td>\n",
       "      <td>-86.986511</td>\n",
       "      <td>AL_DD</td>\n",
       "      <td>AL</td>\n",
       "      <td>The Decatur Daily</td>\n",
       "      <td>http://www.decaturdaily.com</td>\n",
       "      <td>decatur, alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dothan</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.225517</td>\n",
       "      <td>-85.393631</td>\n",
       "      <td>AL_DE</td>\n",
       "      <td>AL</td>\n",
       "      <td>Dothan Eagle</td>\n",
       "      <td>http://www.dothaneagle.com</td>\n",
       "      <td>dothan, alabama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city country   latitude  longitude   slug state                title  \\\n",
       "0    Anniston     USA  33.696739 -85.823433  AL_AS    AL    The Anniston Star   \n",
       "1  Birmingham     USA  33.518509 -86.804756  AL_BN    AL  The Birmingham News   \n",
       "2     Cullman     USA  34.176857 -86.838188  AL_CT    AL    The Cullman Times   \n",
       "3     Decatur     USA  34.602890 -86.986511  AL_DD    AL    The Decatur Daily   \n",
       "4      Dothan     USA  31.225517 -85.393631  AL_DE    AL         Dothan Eagle   \n",
       "\n",
       "                         website           place_name  \n",
       "0    http://www.annistonstar.com    anniston, alabama  \n",
       "1  http://www.al.com/birmingham/  birmingham, alabama  \n",
       "2    http://www.cullmantimes.com     cullman, alabama  \n",
       "3    http://www.decaturdaily.com     decatur, alabama  \n",
       "4     http://www.dothaneagle.com      dothan, alabama  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_newspapers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unidentified_map = {}\n",
    "\n",
    "unidentified_places = us_newspapers_with_pop[us_newspapers_with_pop.POPESTIMATE2015.isnull()]\n",
    "\n",
    "for i, row in unidentified_places.iterrows():\n",
    "    matches = (df_population.STNAME == row.state) & (df_population.NAME.str.lower().str.contains(row.city.lower()))\n",
    "    if matches.sum() == 0:\n",
    "        continue\n",
    "        \n",
    "    pops = df_population[matches].sort_values('POPESTIMATE2015').iloc[0]\n",
    "    unidentified_map[row.place_name] = (pops.NAME, pops.POPESTIMATE2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 74 unidentified places, we found 0 by looking for substrings.\n"
     ]
    }
   ],
   "source": [
    "print('''Out of {} unidentified places, we found {} by looking for substrings.'''.format(\n",
    "    unidentified_places.shape[0],\n",
    "    len(unidentified_map)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So now 657 out of 731 newspapers have populations.\n",
      "\n",
      "Largest newspapers by population:\n",
      "                        title state\n",
      "295        The New York Times    NY\n",
      "659  Metro - New York Edition    NY\n",
      "656               AM New York    NY\n",
      "658        Impacto Latin News    NY\n",
      "657         Catholic New York    NY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def set_from_map_if_null(row):\n",
    "    if pd.isnull(row.POPESTIMATE2015):\n",
    "        return unidentified_map.get(row.place_name, [np.nan, np.nan])[1]\n",
    "    \n",
    "    return row.POPESTIMATE2015\n",
    "\n",
    "us_newspapers_with_pop['population_est_2015'] = us_newspapers_with_pop.apply(set_from_map_if_null, 1)\n",
    "\n",
    "print('''So now {} out of {} newspapers have populations.\n",
    "\n",
    "Largest newspapers by population:\n",
    "{}\n",
    "'''.format(\n",
    "    us_newspapers_with_pop.population_est_2015.count(),\n",
    "    us_newspapers_with_pop.shape[0],\n",
    "    us_newspapers_with_pop.sort_values('population_est_2015', ascending=False).head(5)[['title', 'state']]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof. Looks like population might not work so well, since large cities often have several, lesser-read newspapers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most headline-y words per day\n",
    "\n",
    "This is a variation on the unigram experiment above, where instead we will compute the percent of page for _all_ words in all newspapers. Then we'll average them together across the newspapers to get the \"most headliney words\".\n",
    "\n",
    "A few variations we'll consider:\n",
    "\n",
    "* We'll run one version where we consider the area given to each word independently, and another one where the bounding box of the entire text box where the word is found is grouped together. In terms of the front page real estate, one approach can be viewed as basically calculating the real estate for individual words, and the other for \"topics\" where topics consist of all the words in the document.\n",
    "* There is going to be a lot of noise from stopwords. \"The\", for instance, will be present in nearly all of the articles. We should perform tf-idf to scale the data first. However, we don't want tf-idf to count newsy words toward the document frequency, so we'll calculate it on a separate corpus first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 2385.7400418737307),\n",
       " ('coal', 2378.2648640964944),\n",
       " ('hot', 2241.2010917647),\n",
       " ('running', 2241.2010917647),\n",
       " ('the', 1337.1773106835635)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, without any idf weighting, we'll calculate the contribution of individual words\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def vocab_weights_by_word(df):\n",
    "    counter = Counter()\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        for word in row.bow:\n",
    "            # we won't multiply by the number of characters to get closer to \"true\" word real estate because we don't\n",
    "            # care about the length of words. but we will divide by the total area of the page to normalize across\n",
    "            # newspapers that are different sizes.\n",
    "            counter[word] += row.avg_character_area \n",
    "    \n",
    "    return counter\n",
    "\n",
    "sorted(vocab_weights_by_word(df_denver_post_latest).items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there needs to be some kind of weighting, or else words like \"by\" will dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll calculate document frequencies across the 10788 articles in the Reuters corpus.\n",
      "\n",
      "The most common words in the corpus are:\n",
      "[('of', 7622), ('the', 6951), ('to', 6944), ('said', 6784), ('and', 6765)]\n",
      "\n",
      "As idfs:\n",
      "[('of', 0.34739560282581505), ('the', 0.43954887115913471), ('to', 0.4405564279194345), ('said', 0.46386750678788152), ('and', 0.4666721436547322)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "doc_freq_counter = Counter()\n",
    "\n",
    "for fid in reuters.fileids():\n",
    "    bow = set(map(operator.methodcaller('lower'), reuters.words(fid)))\n",
    "    bow = bow - set(string.punctuation) - set(string.digits)\n",
    "    doc_freq_counter.update(bow)\n",
    "\n",
    "idfs = {}\n",
    "for word, count in doc_freq_counter.items():\n",
    "    idfs[word] = np.log(float(len(reuters.fileids())) / count)\n",
    "    \n",
    "print('''We'll calculate document frequencies across the {} articles in the Reuters corpus.\n",
    "\n",
    "The most common words in the corpus are:\n",
    "{}\n",
    "\n",
    "As idfs:\n",
    "{}\n",
    "'''.format(\n",
    "    len(reuters.fileids()),\n",
    "    sorted(doc_freq_counter.items(), key=operator.itemgetter(1), reverse=True)[:5],\n",
    "    sorted(idfs.items(), key=operator.itemgetter(1))[:5],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words in the latest Denver Post by aggregate word \"real estate\",\n",
      "weighted by inverse document frequency:\n",
      "[('hot', 14598.289587398463),\n",
      " ('coal', 12231.546900414951),\n",
      " ('running', 11355.453183146501),\n",
      " ('debating', 4733.3371356535126),\n",
      " ('marijuana', 4733.3371356535126),\n",
      " ('floral', 4632.6843716517942),\n",
      " ('missile', 3961.990959304801),\n",
      " ('launch', 3921.7252721414152),\n",
      " ('tradition', 3879.3145661941321),\n",
      " ('korea', 3438.5855404812869)]\n",
      "\n",
      "With word areas taken into consideration (longer words get weighted higher):\n",
      "[('running', 79488.172282025509),\n",
      " ('coal', 48926.187601659803),\n",
      " ('hot', 43794.868762195387),\n",
      " ('marijuana', 42600.034220881615),\n",
      " ('debating', 37866.697085228101),\n",
      " ('tradition', 34913.831095747191),\n",
      " ('floral', 27796.106229910765),\n",
      " ('missile', 27733.936715133605),\n",
      " ('launch', 23530.351632848495),\n",
      " ('failure', 22874.952740841112)]\n",
      "\n",
      "Using the area of the entire block:\n",
      "[('denver', 0.56869727734086761),\n",
      " ('coal', 0.47079389655563725),\n",
      " ('sevenfold', 0.4393223319727303),\n",
      " ('pristine', 0.4393223319727303),\n",
      " ('thundering', 0.4393223319727303),\n",
      " ('carved', 0.4393223319727303),\n",
      " ('environmentalists', 0.4393223319727303),\n",
      " ('laid-off', 0.4393223319727303),\n",
      " ('colorado’s', 0.4393223319727303),\n",
      " ('steamed', 0.4393223319727303)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again, this time with idf weighting\n",
    "\n",
    "def vocab_weights_by_word(df, idf=None, method='by_char'):\n",
    "    '''Methods:\n",
    "    `by_char`: Average character size of the textbox in which a string is embedded\n",
    "    `by_word_area`: Average character size * len of string\n",
    "    `by_block`: Area of block in which string is embedded'''\n",
    "    if method not in ['by_char', 'by_word_area', 'by_block']:\n",
    "        raise ArgumentError('method needs to be one of \"by_char\", \"by_word_area\", \"by_block\"')\n",
    "        \n",
    "    counter = Counter()\n",
    "    \n",
    "    max_idf = max(idf.values()) # used for missing values\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        for word in set(row.bow) - set(string.punctuation) - set(string.digits):\n",
    "            # we won't multiply by the number of characters to get closer to \"true\" word real estate because we don't\n",
    "            # care about the length of words. but we will divide by the total area of the page to normalize across\n",
    "            # newspapers that are different sizes.\n",
    "            \n",
    "            if method in ['by_char', 'by_word_area']:\n",
    "                weight = row.avg_character_area\n",
    "\n",
    "                if method == 'by_word_area':\n",
    "                    weight *= len(word)\n",
    "            elif method == 'by_block':\n",
    "                weight = row.percent_of_page\n",
    "            \n",
    "            if idf:\n",
    "                weight *= idf.get(word, max_idf)\n",
    "                \n",
    "            counter[word] += weight\n",
    "            \n",
    "    \n",
    "    return counter\n",
    "\n",
    "print('''The top words in the latest Denver Post by aggregate word \"real estate\",\n",
    "weighted by inverse document frequency:\n",
    "{}\n",
    "\n",
    "With word areas taken into consideration (longer words get weighted higher):\n",
    "{}\n",
    "\n",
    "Using the area of the entire block:\n",
    "{}\n",
    "\n",
    "'''.format(\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs).items(), key=operator.itemgetter(1), reverse=True)[:10]),\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs, method='by_word_area').items(), key=operator.itemgetter(1), reverse=True)[:10]),\n",
    "    pprint.pformat(sorted(vocab_weights_by_word(df_denver_post_latest, idfs, method='by_block').items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better document frequencies\n",
    "\n",
    "The Reuters corpus is only ~10k documents. Instead, let's reverse engineer the document frequencies from the words in a word2vec model of Google News and Zipf's Law.\n",
    "\n",
    "(Skip to other window, where I did this, and found the results to be lackluster.)\n",
    "\n",
    "I requested access to the Yahoo News n-grams corpus. Otherwise, may need to be creative.\n",
    "\n",
    "For now, let's incorporate the document frequencies from the articles themselves in the dataset. The more days we gather, the more we'll be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_idfs(docs):\n",
    "    article_word_doc_counts = Counter()\n",
    "    \n",
    "    for doc in docs:\n",
    "        article_word_doc_counts.update(row.bow)\n",
    "\n",
    "    article_idfs = {}\n",
    "    for word, count in article_word_doc_counts.items():\n",
    "        article_idfs[word] = np.log(float(len(docs)) / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_idfs = make_idfs(df_us.bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of these two different idf datasets:\n",
      "\n",
      "Reuters: 31046\n",
      "Front pages: 106215\n",
      "\n",
      "Most common front page words:\n",
      "[('the', 1.444091346425928),\n",
      " ('and', 1.8025251079432529),\n",
      " ('for', 2.0766460363469013),\n",
      " ('that', 2.5881162153578186),\n",
      " ('with', 2.5939635817934139),\n",
      " ('said', 2.7227947287862069),\n",
      " ('page', 2.7242083785936058),\n",
      " ('see', 2.7408503349043318),\n",
      " ('from', 2.8104198698099769),\n",
      " ('was', 2.8261342322050069)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''Vocabulary size of these two different idf datasets:\n",
    "\n",
    "Reuters: {}\n",
    "Front pages: {}\n",
    "\n",
    "Most common front page words:\n",
    "{}\n",
    "'''.format(\n",
    "    len(idfs),\n",
    "    len(article_idfs),\n",
    "    pprint.pformat(sorted(article_idfs.items(), key=operator.itemgetter(1))[:10])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding \"front-page-est\" words\n",
    "\n",
    "By combining the results of running all of the newspapers on a given day through the method above, we attempt to find the words most representative of front pages across the country on any particular day.\n",
    "\n",
    "We'll run it using all three of the different methods we have for weighting words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  225\n",
      ".....Top results with word area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('easter', 18613.079685581564),\n",
       " ('sunday', 10088.005545237518),\n",
       " ('happy', 9840.0643489703743),\n",
       " ('guguguugggguguguuuggggguguguugguuugggugguggguuguguuuugugguguuuuuuuuuuuuuuuuuuuuuuuuuuuitarrs',\n",
       "  7560.1167368052365),\n",
       " ('the', 5494.2195323558781),\n",
       " ('ourourourourourourourourour', 5180.1484222859299),\n",
       " ('enquirer', 5138.9624308416396),\n",
       " ('advocate', 5088.9972801072263),\n",
       " ('jubilation', 4695.557744835517),\n",
       " ('bird', 3663.866081493743)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_word_area')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with word area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  225\n",
      ".....Top results with character area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('easter', 3102.1799475969301),\n",
       " ('happy', 1968.012869794075),\n",
       " ('the', 1831.406510785293),\n",
       " ('sunday', 1681.3342575395889),\n",
       " ('for', 1161.4890319064421),\n",
       " ('bird', 915.96652037343574),\n",
       " ('and', 848.04174262098059),\n",
       " ('day', 839.61806427076419),\n",
       " ('trg', 773.49003834668872),\n",
       " ('with', 758.62067593992685)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_char')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with character area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers:  225\n",
      ".....Top results with block area:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('and', 0.2530149956214649),\n",
       " ('the', 0.24881876117833573),\n",
       " ('for', 0.22261071681588893),\n",
       " ('that', 0.21841477964153333),\n",
       " ('with', 0.20386716198585889),\n",
       " ('said', 0.19573384805281524),\n",
       " ('but', 0.17964328527481582),\n",
       " ('from', 0.17576985949193535),\n",
       " ('was', 0.1698074213670718),\n",
       " ('are', 0.16296656858932562)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab_weights = {}\n",
    "todays_papers = df_us_3plus[df_us_3plus.date == df_us_3plus.date.max()]\n",
    "print('Total papers: ', todays_papers.slug.nunique())\n",
    "\n",
    "for i, (slug, paper) in enumerate(todays_papers.groupby('slug')):\n",
    "    if i % 50 == 0:\n",
    "        print('.', end='')\n",
    "    all_vocab_weights[slug] = vocab_weights_by_word(paper, article_idfs, method='by_block')\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(all_vocab_weights.values())\n",
    "\n",
    "print('Top results with block area:')\n",
    "sorted(zip(vectorizer.feature_names_, X.mean(axis=0)), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! So it looks like:\n",
    "\n",
    "* all the methods give too much weight to frequency of appearance, vs. rareness of word\n",
    "* this is especially evident with block area, since every time \"the\" shows up, the entire area of the block is counted\n",
    "* the word area weight gives some interesting results but they get skewed by extremely large banners (the enquirer, the advocate, \"sunday\", etc)\n",
    "\n",
    "So that means the next steps are:\n",
    "* remove stopwords\n",
    "* remove names of newspapers from themselves\n",
    "* find way to penalize common words even more than idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "\n",
    "Bringing in the approach from the \"Front Page Heatmap\" notebook, we can try to visualize the prominence of certain words across front pages today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sam/workspace/news/analysis/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_us_3plus['page_height_round'] = df_us_3plus.page_height.apply(int)\n",
    "df_us_3plus['page_width_round'] = df_us_3plus.page_width.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAJyCAYAAABEwUIcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nodEclHsWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbeb470>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "def plot_word(dataframe, word, date=None, paper=None):\n",
    "    title = 'Appearances of {}'.format(word)\n",
    "    if date:\n",
    "        dataframe = dataframe[dataframe.date == date]\n",
    "        title += ' on {}'.format(date)\n",
    "    \n",
    "    if paper:\n",
    "        dataframe = dataframe[dataframe.slug == utils.slug_for_newspaper(paper)]\n",
    "        title += ' on {}'.format(paper)\n",
    "        \n",
    "    relevant_df = dataframe[dataframe.bow.apply(lambda bow: word in bow)]\n",
    "    grids = []\n",
    "    for (date, slug), paper in relevant_df.groupby(['date', 'slug']):\n",
    "        grids.append(utils.make_intensity_grid(relevant_df, relevant_df.page_height_round.max(), relevant_df.page_width_round.max()))\n",
    "    \n",
    "    avg_intensity = sum([x / len(grids) for x in grids])\n",
    "    return utils.plot_intensity(avg_intensity, title)\n",
    "\n",
    "plot_word(df_us_3plus, 'syria')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
